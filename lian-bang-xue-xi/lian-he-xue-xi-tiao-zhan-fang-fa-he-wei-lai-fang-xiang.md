# 联合学习:挑战、方法和未来方向

## 摘要

联邦学习包括在远程设备或竖井数据中心\(如移动电话或医院\)上训练统计模型，同时保持数据本地化。在异构和潜在的大规模网络中的训练引入了新的挑战，需要从根本上背离大规模机器学习、分布式优化和隐私保护数据分析的标准方法。在这篇文章中，我们讨论了联邦学习的独特特征和挑战，提供了当前方法的广泛概述，并概述了与广泛的研究社区相关的未来工作的几个方向。

## 1 引言

移动电话、可穿戴设备和自动驾驶汽车只是现代分布式网络中的一小部分，每天都会产生大量数据。由于这些设备不断增长的计算能力——加上对传输私人信息的关注——本地存储数据和将网络计算推向边缘越来越有吸引力。

边缘计算的概念并不新鲜。事实上，跨分布式、低功耗设备计算简单查询是一个长达数十年的研究领域，已经在传感器网络查询处理、边缘计算和雾计算的范围内进行了探索\[12,29,40,49,74\]。最近的研究也考虑了集中训练机器学习模型，但在本地服务和存储它们;例如，这是移动用户建模和个性化的常用方法\[60,90\]。

然而，随着分布式网络中设备的存储和计算能力的增长，可以利用每个设备上增强的本地资源。这导致了人们对联邦学习的兴趣不断增长\[75\]，该学习探索直接在远程设备上训练统计模型\[1\]。正如我们在本文中讨论的，在这样的环境中学习与传统的分布式环境有很大的不同——需要隐私、大规模机器学习和分布式优化等领域的基本进展，并在不同领域的交叉领域提出了新问题，如机器学习和系统\[91\]。

图1:手机下一个单词预测任务的联合学习应用示例。为了保护文本数据的隐私并减少网络上的压力，我们寻求以分布式方式训练预测器，而不是将原始数据发送到中央服务器。在这种设置中，远程设备定期与中央服务器通信，以学习全局模型。在每一轮通信中，选定的手机的一个子集对其分布不同的用户数据进行本地训练，并将这些本地更新发送到服务器。合并更新之后，服务器然后将新的全局模型发送回设备的另一个子集。这种迭代训练过程在网络中持续进行，直到达到收敛或满足某个停止准则。

联邦学习方法已被主要的服务提供商部署\[11,124\]，并在支持隐私敏感的应用程序中发挥了关键作用，这些应用程序的训练数据分布在边缘\[例如。， 5, 46, 51, 89, 105, 127, 139\]。潜在的应用包括：情感学习，语义定位，或手机用户的活动；适应自动驾驶汽车中的行人行为；以及预测可穿戴设备引发的心脏病等健康事件\[6,52,84\]。下面我们讨论几个联邦学习的规范应用：

* **智能手机。**通过在一个巨大的手机池学习用户行为，统计模型可以为下一个词预测、人脸检测和语音识别等应用提供动力\[46,89\]。然而，用户可能不愿意分享他们的数据，以保护他们的个人隐私或节省他们手机的有限的带宽/电池电力。联邦学习有潜力在智能手机上启用预测功能，而不会减少用户体验或泄露私人信息。图1描述了一个这样的应用程序，在这个应用程序中，我们的目标是基于用户的历史文本数据\[46\]在大规模移动电话网络中学习下一个单词预测器。
* **组织。**组织或机构也可以被视为联合学习环境中的“设备”。例如，医院是一些组织，它们包含大量用于预测性医疗保健的患者数据。然而，医院在运营中有严格的隐私惯例，可能会面临法律、行政或道德方面的限制，要求数据保留在本地。联邦学习是\[52\]这些应用程序的一个有前途的解决方案，因为它可以减少网络的压力，并允许各种设备/组织之间的私人学习。
* **物联网。**现代物联网网络，如可穿戴设备、自动驾驶汽车或智能家居，可能包含大量传感器，使它们能够实时收集、反应和适应传入数据。例如，一个自动驾驶车队可能需要最新的交通，建筑或行人行为模型来安全操作。然而，由于数据的私有性质和每个设备的有限连接，在这些场景中构建聚合模型可能很困难。联邦学习方法可以帮助训练模型，有效地适应这些系统中的变化，同时保持用户隐私\[84,98\]。

## 1.1 问题公式化

规范联邦学习问题涉及从存储在数千万到数百万远程设备上的数据中学习单个的、全局的统计模型。我们的目标是在设备生成的数据在本地存储和处理，只有中间更新定期与中央服务器通信的约束下学习这个模型。特别是，目标通常是最小化以下目标函数：

其中，m为设备总数，pk≥0，∑kpk= 1, Fkis为第k个设备的局部目标函数。局部目标函数通常定义为局部数据的经验风险，即Fk\(w\) =1 nk∑nk jk=1fjk\(w;xjk,yjk\)，其中nkis为局部可用的样本数。用户定义术语pkk指定每个设备的相对影响，两个自然设置pk=1或pk=nk n，其中n =∑knkis为样本总数。我们将在整篇文章中引用问题\(1\)，但是，正如下面所讨论的，我们注意到其他目标或建模方法可能是适当的，这取决于感兴趣的应用程序。

## 1.2 核心挑战

接下来，我们将描述与解决\(1\)中提出的分布式优化问题相关的四个核心挑战。这些挑战使联邦设置不同于其他经典问题，如数据中心设置中的分布式学习或传统的私有数据分析。

**挑战1：昂贵的通讯费用。**在联邦网络中，通信是一个关键的瓶颈，再加上发送原始数据时的隐私问题，使得在每个设备上生成的数据必须保持在本地。事实上，联邦网络可能由大量设备组成，例如数百万部智能手机，网络中的通信可能比本地计算慢许多个数量级\[50,115\]。为了适应数据模型生成的联合网络设备，因此有必要开发communication-efficient迭代方法发送小消息或模型更新训练过程的一部分，而不是通过网络发送整个数据集。要在这种情况下进一步减少通信，需要考虑的两个关键方面是：\(i\)减少通信轮的总数，或\(ii\)减少每轮传输信息的大小。

**挑战2：系统异构性。**联邦网络中每个设备的存储、计算和通信能力可能因硬件\(CPU、内存\)、网络连接性\(3G、4G、5G、wifi\)和电源\(电池电量\)的不同而不同。此外，每个设备上的网络大小和系统相关的限制通常导致只有一小部分设备同时处于活动状态，例如，在一个百万设备的网络\[11\]中有数百个活动设备。每个设备也可能是不可靠的，活跃设备在给定迭代中由于连接或能量限制而退出的情况并不罕见。这些系统级的特征极大地加剧了诸如掉队缓解和容错等挑战。因此，开发和分析的联邦学习方法必须：\(i\)预期低参与度，\(ii\)容忍异构硬件，以及\(iii\)对网络中的丢弃设备具有鲁棒性。

**挑战3：统计上的异质性。**设备经常以不同的方式在网络上生成和收集数据，例如，移动电话用户在下一个单词预测任务的上下文中使用不同的语言。此外，设备之间的数据点数量可能会有很大的差异，并且可能存在一个捕获设备及其相关分布之间的关系的底层结构。这种数据生成范式违反了分布式优化中经常使用的独立和同分布\(I.I.D.\)假设，增加了掉线的可能性，并可能增加建模、分析和评估方面的复杂性。的确，虽然\(1\)中的规范联邦学习问题的目标是学习单个全局模型，但也存在其他替代方法，如通过多任务学习框架\[cf.\]同时学习不同的局部模型。106\]。在这方面，联邦学习和元学习的主要方法之间也有密切的联系\[64\]。多任务和元学习视角都支持个性化或设备特定的建模，这通常是处理数据统计异质性的更自然的方法。

**挑战四：隐私问题。**最后，隐私通常是联邦学习应用程序的主要关注点。联邦学习通过共享模型更新，例如梯度信息，而不是原始数据，向保护每个设备上生成的数据迈出了一步\[17,31,33\]。然而，在整个训练过程中，模型更新的通信仍然可以向第三方或中央服务器披露敏感信息\[76\]。虽然最近的方法旨在使用安全多方计算或差分隐私等工具增强联邦学习的隐私性，但这些方法通常以降低模型性能或系统效率为代价提供隐私。从理论和经验上理解和平衡这些权衡，是实现私有联合学习系统的一个相当大的挑战。

本文的其余部分组织如下。在第2节中，我们将介绍以前和当前的工作，这些工作旨在解决所讨论的联邦学习的四个挑战。在第三部分，我们概述了未来研究的几个有希望的方向。

