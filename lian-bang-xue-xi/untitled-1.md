# index

原文：[联邦学习的进展和有待解决的问题](https://export.arxiv.org/pdf/1912.04977)

## 摘要

联邦学习\(FL\)是一种机器学习方法，许多客户端\(例如移动设备或整个组织\)在中央服务器\(例如服务提供商\)的编排下协同训练一个模型，同时保持训练数据分散。FL体现了集中数据收集和最小化的原则，可以减轻许多由传统的、集中的机器学习和数据科学方法造成的系统性隐私风险和成本。由于FL研究的爆炸性增长，本文讨论了最近的进展，并提出了广泛的开放式问题和挑战。

## 1 介绍

联邦学习\(FL\)是一种机器学习方法，许多客户端\(例如移动设备或整个组织\)在中央服务器\(例如服务提供商\)的编排下协同训练一个模型，同时保持训练数据分散。它体现了集中收集和数据最小化的原则，可以减轻许多由传统的、集中的机器学习导致的系统性隐私风险和成本。从研究和应用的角度来看，这一领域最近受到了极大的关注。本文描述了联邦学习方法的定义特征和挑战，强调了重要的实践约束和注意事项，并列举了一系列有价值的研究方向。这项工作的目标是突出具有重大理论和实践兴趣的研究问题，并鼓励对可能有重大现实影响的问题的研究。

联邦学习这个术语是由McMahan等人在2016年引入的\[337\]:“我们将我们的方法称为联邦学习，因为学习任务是通过由中央服务器协调的参与设备\(我们称之为客户端\)的松散联邦来解决的。”跨大量通信带宽有限的不可靠设备的不平衡和非IID\(相同和独立分布\)数据分区被引入作为定义方面的挑战。

在联邦学习这个术语出现之前，就已经有了一些重要的相关工作。许多研究团体\(包括密码学、数据库和机器学习\)追求的一个长期目标是分析和学习分布在许多所有者之间的数据，而不暴露这些数据。在加密数据上计算的加密方法始于20世纪80年代早期\[396,492\]，Agrawal和Srikant\[11\]和Vaidya等人\[457\]是早期尝试使用集中式服务器从本地数据中学习并同时保护隐私的例子。相反，即使引入了联邦学习这个术语，我们也没有发现任何一项工作可以直接解决FL的所有挑战。因此，联邦学习这个术语为一组特征、约束和挑战提供了方便的简写，这些特征、约束和挑战经常同时出现在去中心化数据的应用ML问题中，而在去中心化数据中，隐私至关重要。

本文源自于谷歌西雅图办公室于2019年6月17 - 18日举办的联邦学习与分析研讨会。在这次为期两天的活动中，对联邦学习领域中许多开放式挑战进行广泛调查的必要性变得清晰起来。

讨论的许多问题的一个关键属性是，它们本质上是跨学科的——解决这些问题可能不仅需要机器学习，还需要来自分布式优化、密码学、安全、差分隐私、公平、压缩感知、系统、信息理论、统计学等方面的技术。许多最棘手的问题都处在这些领域的交叉点上，因此我们相信，协作将对持续的进展至关重要。这项工作的目标之一是强调这些领域的技术可以结合的方式，提出有趣的可能性和新的挑战。

自从术语联邦学习最初引入的重点是移动和边缘设备应用程序\(337、334\),对FL应用到其他应用程序的兴趣大大增加,其中一些可能只涉及少量的相对可靠的客户,例如多个组织合作训练模型。我们分别将这两个联邦学习方法称为“跨设备”和“跨竖井”。鉴于这些变化，我们提出了一个更广泛的联邦学习定义：

**联邦学习**是一种机器学习方法，在中央服务器或服务提供商的协调下，多个实体\(客户端\)协作解决机器学习问题。每个客户的原始数据存储在本地，不交换或转移;相反,有针对性的更新是狭义的更新，以包含手头特定学习任务所需的最低信息;在数据最小化服务中，尽可能早地执行聚合。我们注意到，这个定义将联邦学习与第2.1节中讨论的完全分散\(点对点\)学习技术区分开来。

虽然隐私保护数据分析的研究已经超过50年，但只有在过去的10年才有大规模的解决方案被广泛部署\(例如\[177,154\]\)。跨设备联邦学习和联邦数据分析正在应用于消费数字产品中。谷歌在Gboard移动键盘\[376,222,491,112,383\]以及Pixel手机\[14\]和Android Messages\[439\]中广泛使用了联邦学习。虽然谷歌是跨设备FL的先驱，但对这种方法的兴趣现在更广泛了，例如:苹果在iOS 13的\[25\]中使用跨设备FL，用于QuickType键盘和“Hey Siri”\[26\]的声音分类器;doc.ai 正在为医学研究开发跨设备的FL解决方案\[149\]，而Snips已经探索了用于热点词检测的跨设备FL\[298\]。

跨竖井应用也被提出或描述在无数领域，包括再保险的金融风险预测\[476\]、药品发现\[179\]、电子健康记录挖掘\[184\]、医疗数据分割\[15,139\]和智能制造\[354\]。

对联邦学习技术不断增长的需求导致了大量的工具和框架的出现。这些包括TensorFlow Federated \[38\]， Federated AI Technology Enabler \[33\]， PySyft \[399\]， Leaf \[35\]， PaddleFL\[36\]和Clara Training Framework \[125\];更多细节请参见附录a。商业数据平台联合联邦学习是由成熟的技术公司以及小型初创公司开发的。

表1对比了跨设备和跨竖井联邦学习与跨一系列轴的传统单数据中心分布式学习。这些特征建立了许多实际联邦学习系统通常必须满足的约束，因此既可以激励联邦学习，也可以告知联邦学习中的开放式挑战。它们将在接下来的章节中详细讨论。

![image-20210426105143687](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/table-1.png)

表1：联邦学习算法与数据中心中分布式学习的典型特征\(例如\[150\]\)。跨设备和跨竖井联邦学习是FL领域的两个例子，但并不打算详尽无遗。FL的主要定义特征以粗体突出，但其他特征在决定哪些技术适用时也是至关重要的。

这两种FL的变体被称为代表性和重要的例子，但不同的FL方法可能有这些特征的不同组合。在本文的其余部分，我们考虑跨设备的FL算法，除非另有说明，尽管许多问题也适用于其他FL算法。第2节专门讨论了许多其他变体和应用程序中的一些。

接下来，我们将更详细地考虑跨设备联邦学习，重点关注该技术典型大规模部署中常见的实际方面;Bonawitz等人\[81\]为特定的生产系统提供了更多的细节，包括对特定架构选择和考虑事项的讨论。

### 1.1 跨设备联邦学习算法

本节采用应用透视图，与前一节不同的是，不尝试定义透视图。相反，我们的目标是描述跨设备FL中的一些实际问题，以及它们如何适应更广泛的机器学习开发和部署生态系统。希望为接下来的开放问题提供有用的背景和动机，同时帮助研究人员估计在现实世界系统中部署一种特殊的新方法有多简单。在考虑FL训练过程之前，我们先画一个模型的生命周期草图。

#### 1.1.1 联邦学习模式的生命周期

FL过程通常是由为特定应用程序开发模型的模型工程师驱动的。例如，自然语言处理领域的专家可以开发一个用于虚拟键盘的下一个单词预测模型。图1显示了主要组件和参与者。在高层次上，典型的工作流是：

![image-20210426114546464](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/figure-1.png)

图1：经过fl训练的模型的生命周期和联邦学习系统中的各种参与者。第4节从威胁模型的角度重新讨论了该图。

1. **问题识别**：模型工程师识别一个需要用FL解决的问题。
2. **客户端检测**：如果需要，客户端\(例如手机上运行的应用程序\)被检测到本地存储必要的训练数据\(有时间和数量限制\)。在很多情况下，应用程序已经存储了这些数据\(例如，一个短信应用程序必须存储短信，一个照片管理应用程序已经存储照片\)。然而，在某些情况下，可能需要维护额外的数据或元数据，例如用户交互数据，为监督学习任务提供标签。
3. **仿真原型\(可选\)**：模型工程师可以使用代理数据集在FL模拟中对模型架构进行原型化并测试学习超参数。
4. **联邦模型训练**：启动多个联邦训练任务来训练模型的不同变体，或使用不同的优化超参数。
5. **\(联邦\)模型评估**：在任务得到充分训练之后\(通常是几天，如下\)，将对模型进行分析并选择好的候选者。分析可能包括在数据中心的标准数据集上计算的指标，或者联邦评估，其中将模型推到保留的客户端，对本地客户数据进行评估。
6. **部署**：最后，一旦一个好的模型被选中，它将经历一个标准的模型启动过程，包括手动质量保证，实时的A /B测试\(通常是在一些设备上使用新模型，在其他设备上使用上一代模型来比较它们的体内性能\)，以及分段推出\(以便在影响太多用户之前发现和回滚不良行为\)。模型的特定启动过程是由应用程序的所有者设置的，通常与模型如何训练无关。换句话说，这个步骤同样适用于经过联邦学习或传统数据中心方法训练的模型。

FL系统面临的主要实际挑战之一是使上述工作流程尽可能简单，理想地接近ML系统实现集中训练的易用性。虽然本文主要关注联邦训练，但还有许多其他组件，包括联邦分析任务，如模型评估和调试。改进这些是第3.4节的重点。现在，我们更详细地考虑单个FL模型的训练\(上面的第4步\)。

#### 1.1.2 典型的联邦训练过程

我们现在考虑一个FL训练模板，它包含McMahan等人\[337\]的联邦平均算法和许多其他算法；同样，变化是可能的，但这提供了一个共同的起点。

服务器\(服务提供者\)通过重复以下步骤来协调训练过程，直到训练停止\(由监控训练过程的模型工程师决定\)：

1. **客户端选择：**来自一组满足资格要求的客户端的服务器示例。例如，为了避免影响设备的用户，移动电话可能只有在插入、未计量的wi-fi连接和空闲的情况下才会检查到服务器。
2. **广播：**选定的客户端从服务器下载当前的模型权重和一个训练程序\(例如TensorFlow图\[2\]\)。
3. **客户端计算：**每个选定的设备通过执行训练程序本地计算对模型的更新，例如，训练程序可以在本地数据上运行SGD\(如Federated平均化\)。
4. **聚合：**服务器对设备的更新进行聚合。为了提高效率，一旦有足够数量的设备报告了结果，可能会在此时删除掉队的设备。这一阶段也是许多其他技术的集成点，这些技术将在后面讨论，可能包括：用于增加隐私的安全聚合，用于通信效率的聚合的有损压缩，以及用于差分隐私的噪声添加和更新剪辑。
5. **模型更新：**服务器基于从参与当前轮的客户端计算的聚合更新本地更新共享模型。

表2给出了移动设备上典型联邦学习应用程序中涉及的数量的典型数量级。

![image-20210426143501496](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/table-2.png)

表2：典型的跨设备联邦学习应用程序的数量级大小。

分离客户端计算、聚合和模型更新阶段并不是联邦学习的严格要求，而且它确实排除了某些算法类，例如异步SGD，在使用其他客户端的更新进行任何聚合之前，每个客户端的更新都立即应用于模型。这种异步方法可能简化系统设计的某些方面，并且从优化的角度来看也是有益的\(尽管这一点还有待讨论\)。然而，上述方法在区分不同研究方向的关注点方面有很大的优势：压缩、差分隐私和安全多方计算的进步可以用于标准原语，如求和或通过去中心化更新的方法，然后与任意优化或分析算法组成，只要这些算法以聚合原语的形式表示。

同样值得强调的是，在两个方面，FL的训练过程不应该影响用户体验。首先，如上所述，尽管模型参数通常在每一轮联邦训练的广播阶段发送到一些设备，但这些模型只是训练过程中短暂的一部分，不用于向用户显示“实时”预测。这是至关重要的，因为训练ML模型是具有挑战性的，而且一个超参数的错误配置可能产生一个做出错误预测的模型。相反，用户可见的模型使用被推迟到模型生命周期的第6步中详细描述的推出过程中。第二，训练本身对用户是不可见的——正如在客户端选择中所描述的，训练不会使设备变慢或耗尽电池，因为它只在设备空闲和连接电源时执行。然而，这些限制所带来的有限可用性直接导致开放式研究挑战，如半循环数据可用性和客户选择中可能存在的偏见，我们将在后面讨论。

### 1.2 联邦学习研究

本文的其余部分调查了许多由现实世界联邦学习算法的约束和挑战引起的开放问题，从医院系统的医疗数据训练模型到使用数亿移动设备进行训练。不用说，大多数致力于联邦学习问题的研究人员很可能不会部署生产的FL系统，也无法接触到数以百万计的实际设备。这就导致了激励工作的实际算法和模拟中进行的实验之间的关键区别，这些实验为激励问题的给定方法的适用性提供了证据。

这使得FL研究从实验的角度与其他ML领域有些不同，导致在进行FL研究时需要额外的考虑。特别是，当着重开放问题时，如果可能的话，我们也尝试指出可以在模拟中测量的相关性能指标，数据集的特征将使它们更能代表现实世界的性能，等等。模拟的需要也有分支的介绍FL研究。虽然我们无意成为权威或绝对的，但我们为介绍FL研究提出了以下适度的建议，以解决我们所描述的开放性问题：

* 如表1所示，FL算法可以包含广泛的问题。与算法和目标已经确定的领域相比，准确描述感兴趣的特定FL算法的细节是重要的，特别是当所提议的方法做出的假设可能不适用于所有算法时\(例如，有状态的客户参与所有回合\)。
* 当然，为了使研究可重复，任何模拟的细节都应该被展示出来。但是，为了有效地证明模拟问题的成功意味着对现实世界目标的有用进展，解释模拟是为了捕捉现实世界算法的哪些方面\(而不是哪些方面\)也是很重要的。我们希望本文中的指导将对此有所帮助。
* 在FL中，隐私和通信效率始终是首要问题，即使实验是在使用公共数据的单机上运行的模拟。与其他类型的ML相比，更重要的是，对于任何提出的方法来说，明确计算发生在哪里以及传递什么是很重要的。

联邦学习模拟软件库和标准数据集可以帮助缓解开展有效的FL研究的挑战；附录A总结了一些当前可用的选项。为不同的联邦学习算法\(跨设备和跨竖井\)开发标准评估指标和建立标准基准数据集仍然是当前工作的重要方向。

### 1.3 结构

第2节以表1的思想为基础，探讨了跨设备方法之外的其他FL方法和问题。第三部分将讨论关于如何提高联邦学习的效率和有效性的核心问题。第4节对威胁模型进行了仔细考虑，并考虑了实现严格隐私保护目标的一系列技术。与所有机器学习系统一样，在联邦学习应用程序中，可能存在操纵被训练模型的动机，各种类型的失败是不可避免的;这些挑战将在第5节中讨论。最后，我们在第6节中讨论了提供公平和无偏见模型的重要挑战。

## 2 提出核心FL假设：新兴算法和场景的应用

在本节中，我们将讨论与前一节中讨论的主题相关的研究领域。尽管这不是本文的重点，但这些领域的进展可以激励下一代生产系统的设计。

### 2.1 完全分散/点对点分布式学习

在联邦学习中，中央服务器协调训练过程，并接收所有客户端的贡献。因此，服务器是一个中心角色，它也可能代表一个单点故障。虽然大型公司或组织可以在某些应用程序场景中扮演这个角色，但在更具协作性的学习场景中，可靠且功能强大的中央服务器可能并不总是可用或可取的\[459\]。此外，当客户端数量非常大时，服务器甚至可能成为瓶颈，Lian等人\[305\]证明了这一点\(尽管这可以通过仔细的系统设计来减轻，例如\[81\]\)。

完全去中心化学习的关键思想是用个人客户端之间的对等通信来取代与服务器的通信。通信拓扑表示为一个连通图，其中节点为客户端，一条边表示两个客户端之间的通信通道。网络图通常选择最小最大度的稀疏网络图，使得每个节点只需要向少量节点发送或接收消息;这与服务器-客户端体系结构的星形图形成了对比。在完全去中心化的算法中，一轮对应于每个执行本地更新并与图2中的邻居交换信息的客户端。在机器学习的背景下，本地更新通常是一个本地\(随机\)梯度步骤，通信包括与邻居的本地模型参数的平均。请注意，不再像标准联邦学习中那样有模型的全局状态，但是可以将过程设计为所有本地模型收敛到所需的全局解决方案，也就是说，各个模型逐渐达成共识。虽然多智能体优化在控制领域有着悠久的历史，但最近人们在机器学习中考虑了SGD的完全分散变种和其他优化算法，以提高数据中心\[29\]的可伸缩性，以及用于分散的设备网络\[127,459,443,59,278,291,173\]。他们考虑了无向网络图，尽管在\[29,226\]中也研究过有向网络的情况\(编码可能出现在社会网络或数据市场等现实场景中的单向通道\)。

值得注意的是，即使在上面概述的分散算法中，一个中央权威机构仍然可能负责设置学习任务。例如，考虑以下问题：在分散的算法中，谁决定要训练什么模型？使用什么算法？超参数是什么？当某些东西不能按预期工作时，谁负责调试？要回答这些问题，仍然需要对中央权威机构中参与的客户有一定程度的信任。或者，决策可以由提出学习任务的客户做出，或者通过共识方案进行协作\(见第2.1.2节\)。

表3提供了联邦学习和对等学习之间的比较。虽然去中心化学习的架构假设与联邦学习不同，但它通常可以应用于类似的问题领域，出现了许多相同的挑战，并且在研究社区中存在显著的重叠。因此，我们在本文中也考虑了分散学习;在这部分挑战明确考虑了特定于分散方法的问题，但是在分散的情况下，其他部分中的许多开放问题也会出现。

![image-20210426145740565](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/table-3.png)

表3：联邦学习和完全分散学习之间的主要区别的比较。注意，与FL一样，分散学习可以进一步划分为不同的用例，其区别类似于表1中比较跨竖井和跨设备FL的区别。

#### 2.1.1 算法面临的挑战

在分散式机器学习方案的真实世界可用性这个话题上，仍有大量重要的算法问题有待解决。有些问题类似于使用中央服务器的联邦学习的特殊情况，而其他挑战则是完全去中心化或无信任的附加副作用。我们在下面概述了一些特定的领域。

**网络拓扑结构和异步对分布式SGD的影响** 完全分散的学习算法应该对有限的客户端可用性\(客户端在执行期间暂时不可用、退出或加入\)和有限的网络可靠性\(可能出现消息丢失\)具有鲁棒性。而对于特殊情况下的广义线性模型，使用对偶结构的方案可以实现这些期望的鲁棒性\[231\]，对于深度学习和SGD，这仍然是一个开放的问题。当网络图是完整的，但消息有固定的被丢弃概率时，Yu等人\[498\]表明，可以达到类似于可靠网络情况下的收敛速度。其他开放式研究问题涉及非IID数据分布、更新频率、有效通信模式和实际收敛时间\[443\]，我们将在下面更详细地概述。

连接良好或更密集的网络鼓励更快的共识，并给出更好的理论收敛速度，这取决于网络图的谱间隙。然而，当数据是IID时，稀疏拓扑在实践中并不一定会损害收敛：这在\[357\]中进行了理论分析。密集的网络通常会导致通信延迟，这种延迟随着节点度的增加而增加。大多数优化理论都没有明确考虑拓扑如何影响运行时，即完成每个SGD迭代所需的时钟时间。Wang等人\[469\]提出了一种基于匹配分解采样的分散SGD方法MATCHA，该方法在保持相同的误差收敛速度的同时，降低了任意给定节点拓扑的每次迭代通信时延。其关键思想是将图拓扑分解为可并行运行的不相交通信链路的匹配，并在每次迭代中仔细选择这些匹配的子集。这个子图序列导致在连接关键链路上有更多的频繁通信\(确保快速的错误收敛\)，在其他链路上有更少的频繁通信\(节省通信延迟\)。

去中心化SGD的算法自然也适合于异步算法，在这种算法中，每个客户端都在随机时间独立活动，从而消除了对全局同步的需求，并潜在地提高了可伸缩性\[127,459,59,29,306\]。

**本地更新分散SGD** 在通信轮前执行多个本地更新步骤的方案的理论分析比那些使用单个SGD步骤的方案\(如小批SGD\)更具挑战性。虽然这也将在后面的3.2节中讨论，但同样的情况也更普遍地适用于我们感兴趣的完全分散方法。依赖于单个本地更新步骤的方案通常被证明在非IID本地数据集的情况下是收敛的\[278,279\]。对于有几个本地更新步骤的情况，\[467,280\]最近提供了收敛性分析。此外，\[469\]为非IID数据案例提供了收敛分析，但针对上述基于匹配分解采样的具体方案。但是，一般来说，理解非IID数据分布下的收敛性以及如何设计一个模型平均策略来实现最快的收敛仍然是一个有待解决的问题。

**个性化和信任机制** 与跨设备FL算法类似，在对单个客户可用的非IID数据分发下的完全分散场景的一个重要任务是设计用于学习个性化模型集合的算法。\[459, 59\]的工作引入了完全分散的算法，通过平滑具有类似任务\(即类似数据分布\)的客户端的模型参数，协作学习每个客户端的个性化模型。Zantedeschi等\[504\]在个性化模型的基础上进一步学习了相似图。在去中心化方法中，一个关键的独特挑战仍然是此类方案对恶意参与者或不可靠数据或标签贡献的健壮性。将激励或机制设计与去中心化学习相结合是一个新兴的重要目标，如果没有可信的中央服务器，这可能更难实现。

**梯度压缩和量化方法** 在潜在的应用中，客户端通常在可用的通信带宽和允许的能源使用方面受到限制。在不影响收敛的情况下，将一些现有的压缩通信方案从集中式协调器方便方法\(见3.5节\)转换和归纳为完全分散算法是一个积极的研究方向\[278,391,444,279\]。一个补充的想法是设计分散的优化算法，自然会产生稀疏更新\[504\]。

**隐私** 在完全分散学习中，一个重要的挑战是防止任何客户端通过其共享更新重构另一个客户端的私有数据，同时为所学习的模型保持良好的实用水平。差分隐私\(见第4节\)是降低此类隐私风险的标准方法。在去中心化联邦学习中，这可以通过让每个客户端在本地添加噪声来实现，如\[239,59\]所做的那样。不幸的是，这种保护本地隐私的做法往往会在公用事业方面付出巨大代价。此外，为了提高标准FL算法见第4.4.3节\)中的隐私权衡而设计的基于安全聚合或安全洗排的分布式方法不容易与完全去中心的算法集成。在隐私和有效去中心化算法之间实现更好的权衡的可能方向是依靠去中心化本身来放大差分隐私保障，例如通过考虑适当放宽本地差分隐私\[146\]。

#### 2.1.2 实际挑战

对于完全分散学习来说，一个正交的问题是如何实际实现它。本节概述了基于分布式账本思想的一系列相关思想，但是还没有探索其他方法。

区块链是在不同用户之间共享的分布式账本，使数字交易\(包括加密货币交易\)成为可能，而没有一个中央权威机构。特别是，智能合约允许在区块链上执行任意代码，区块链本质上是一个大规模复制的最终一致的状态机。就联邦学习而言，使用该技术可以通过使用智能合约进行模型聚合来实现全球服务器的去中心化，其中执行智能合约的参与客户可以是不同的公司或云服务。

然而，在今天的区块链平台上，如以太坊\[478\]，区块链上的数据默认是公开可用的，这可能会阻碍用户参与去中心化联邦学习协议，因为数据保护通常是FL的主要激励因素。为了解决这些问题，修改现有的隐私保护技术以适应去中心化联邦学习的情况是可能的。首先，为了防止参与节点利用单独提交的模型更新，可以使用现有的安全聚合协议。Bonawitz等人提出了一种已经应用于跨设备FL的实用安全聚合协议\[80\]，以协议复杂性为代价有效地处理了退出参与者。另一个替代系统是让每个客户在区块链上存入一笔加密货币，如果他们在执行期间退出，就会受到惩罚。在不需要处理退出的情况下，可以显著简化安全聚合协议。实现安全聚合的另一种方法是使用机密智能合约，例如运行在安全区域内的Oasis协议\[119\]所支持的。这样，每个客户端都可以简单地提交一个加密的本地模型更新，知道模型将通过远程认证在安全硬件中解密和聚合\(参见第4.1节中关于隐私的深入讨论\)。

为了防止任何客户端试图利用全局模型重构另一个客户端的私有数据，FL的客户级差分隐私\[338\]已经被提出。客户级差分隐私是通过在聚合全局模型上添加随机高斯噪声来实现的，该噪声足以隐藏任何单个客户端的更新。在区块链的背景下，每个客户端可以在经过本地梯度下降步骤后，本地添加一定量的高斯噪声，并将模型提交给区块链。应该计算本地噪声尺度，以便区块链上的聚合噪声能够实现与\[338\]中相同的客户端级差分隐私。最后，可以对区块链上的聚合全局模型进行加密，并且只有参与的客户端持有解密密钥，这将保护模型不被公开。

### 2.2 跨竖井联邦学习

与跨设备联邦学习\(见表1\)的特点相比，跨竖井联邦学习在总体设计的某些方面允许更多的灵活性，但同时也提出了实现其他属性可能比较困难的方法。本节将讨论其中的一些差异。

当许多公司或组织共享基于他们所有数据的模型训练动机，但不能直接共享他们的数据时，跨竖井方法可能是相关的。这可能是由于机密性的限制或法律限制，甚至是在单个公司内部，当它们不能在不同地理区域之间集中数据时。这些跨竖井的应用已经引起了大量的关注。

**数据分区** 在跨设备算法中，假设数据按示例进行分区。在跨竖井方法中，除了按示例进行分区外，按特性进行分区也具有实际意义。例如，当处于不同业务的两家公司拥有相同或重叠的客户集时，例如同一城市的一家本地银行和一家本地零售公司。Yang等人也将这种差异称为水平和垂直联合学习\[490\]。

基于特征划分数据的跨竖井FL，与基于实例划分数据的方法相比，采用了非常不同的训练架构。它可能会也可能不会涉及中央服务器作为中立方，客户端根据训练算法的具体情况，交换特定的中间结果，而不是模型参数，以协助其他方的梯度计算;参见例如\[490，第2.4.2节\]。在这种情况下，为了限制其他参与者通过观察训练过程可以推断出的信息量，已经提出了安全多方计算或同态加密等技术的应用。这种方法的缺点是，训练算法通常依赖于所追求的机器学习目标的类型。目前提出的算法包括树\[118\]、线性和逻辑回归\[490,224,316\]和神经网络\[317\]。与联邦平均相似的本地更新\(见第3.2节\)已被提出，以解决特征分区系统的通信挑战\[316\]，并\[238,318\]研究此类系统固有的安全和隐私相关挑战。

联邦迁移学习\[490\]是另一个概念，它考虑了具有挑战性的场景，其中数据各方只在用户空间或特征空间中共享部分重叠，并利用现有的迁移学习技术\[365\]协作构建模型。现有公式仅限于2个客户端的情况。

在跨竖井的FL中，当单个公司由于法律限制而不能集中数据时，或者当具有相似目标的组织希望协作改进模型时，按示例进行分区通常是相关的。例如，不同的银行可以协作训练用于欺诈检测的分类或异常检测模型\[476\]，医院可以构建更好的诊断模型\[139\]，等等。

支持上述应用程序的开源平台是联邦AI技术使能器\(F ATE\)\[33\]。与此同时，IEEE P3652.1联邦机器学习工作组正在致力于联邦人工智能技术框架的标准方法。其他平台包括关注一系列医疗应用的\[125\]和针对企业用例的\[321\]。详见附录A。

**激励机制** 诚信参与的激励机制设计除了发展新的算法技术外，也是一个重要的实践研究问题。这种需求可能出现在跨设备方法中\(例如\[261,260\]\)，但在跨竖井方法中尤其相关，其中的参与者可能同时也是业务竞争对手。奖励可以是货币奖励\[499\]或具有不同表现水平的最终模型\[324\]。在FL参与者之间存在竞争的合作学习情况下，交付与每个客户贡献相称的性能模型的选择尤其相关。客户可能会担心，将他们的数据贡献给训练联合学习模型将使他们的竞争对手受益，后者贡献没有那么多，但最终获得了相同的模型\(即搭便车问题\)。相关目标包括如何在贡献数据所有者之间分配联邦学习模型产生的收益，以维持长期参与，以及如何将激励与对抗敌对数据所有者的决策联系起来，以提高系统安全性，优化数据所有者的参与，提高系统效率。

**差分隐私** 第4.1节中对行动者和威胁模型的讨论在很大程度上也与跨竖井FL相关。然而，针对不同行动者的保护可能有不同的优先级。例如，在许多实际情况下，最终的训练模型将只发布给那些参加训练的人，这使得对“世界其他地方”的关注不那么重要。

另一方面，对于一个具有实际说服力的声明，我们通常需要一个本地差分隐私的概念，因为来自其他客户的潜在威胁可能更重要。在客户端不被认为是重大威胁的情况下，每个客户端可以控制来自多个各自用户的数据，在这样的用户级别上可能需要一个正式的隐私保证。根据应用情况，其他目标也值得追求。这一领域尚未进行系统的勘探。

**张量分解** 一些工作还研究了跨竖井联邦张量分解，其中多个站点\(每个站点都有一组具有相同特征的数据，即水平分区\)通过只与协调服务器共享中间因子，同时保持每个站点的数据私有，共同执行张量分解。在现有的工作中，\[272\]使用了基于交替方向乘子法\(ADMM\)的方法，\[325\]使用弹性平均SGD \(EASGD\)算法提高了效率，并进一步确保了中间因素的差分隐私。

### 2.3 分离学习

与以前侧重于数据分区和通信模式的方法相比，分离学习背后的关键思想\[215,460\]3是在客户端和服务器之间的每层基础上分割模型的执行。这既可以用于训练，也可以用于推理。

在分离学习的最简单配置中，每个客户端计算向前通过一个深度网络，直到一个称为切割层的特定层。切割层的输出，称为粉碎数据，被发送到另一个实体\(服务器或另一个客户端\)，由后者完成剩余的计算。这在不共享原始数据的情况下完成了一轮前向传播。然后，梯度可以以类似的方式从最后一层传播到切割层。切割层上的梯度——并且只有这些梯度——被发送回客户端，在那里完成其余的反向传播。这个过程一直持续到汇聚，而不需要客户端直接访问彼此的原始数据。图2\(a\)显示了这种方法，图2\(b\)显示了这种方法的变体，其中标签也没有与原始数据一起共享。根据特征划分数据的分离学习方法已经在\[101\]中进行了研究。

在一些情况下，\[421\]比较了分离学习和联邦学习的总体通信需求。分离学习在训练中引入了另一个并行性维度，即模型的各个部分之间的并行化，例如客户端和服务器。在\[245,240\]中，作者打破了部分网络之间的依赖关系，通过在不同部分并行化计算来减少总集中训练时间，这一思想在这里也可以适用。然而，如何在边缘设备上实现并行学习仍然是一个有待解决的问题。分离学习还支持将客户端模型组件与最佳的服务器端模型组件进行匹配，以实现模型的自动化选择，如ExpertMatcher\[413\]所示。

不过，通常情况下，传递的值可以揭示有关底层数据的信息。这在多大程度上以及是否可以接受，可能取决于应用程序和配置。NoPeek SplitNN\[462\]是一种分离学习的变体，它通过减少与原始数据的距离相关性\[461,442\]来减少潜在的泄漏，同时保持良好的模型性能通过分类交叉熵损失。其关键思想是最小化原始数据点与通信粉碎数据之间的距离相关性。如果不使用NoPeek SplitNN，被通信的对象可能包含与输入数据高度相关的信息，使用NoPeek SplitNN还允许根据它提供的去相关性相对较早地进行拆分。另一种工程驱动的方法是通过对客户端激活中出现的通道进行专门的修剪\[422\]来最小化分割学习中通信的信息量。总的来说，第4节中的很多讨论也与此相关，而且专门为分离学习提供正式的隐私保证的分析仍然是一个有待解决的问题。

### 2.4 主要总结

联邦学习的动机与许多相关领域的研究有关。

* 完全分散学习\(第2.1节\)无需中央服务器来协调整个计算。除了算法上的挑战，开放的问题是在这个想法的实际实现上，以及在理解建立任务需要什么样的可信任的中央权威上。
* 跨竖井联邦学习\(第2.2节\)承认不同类型的建模约束存在问题，如按示例和/或特征划分的数据，并在为客户制定正式的隐私保证或激励机制时面临不同的关注。
* 分离学习\(第2.3节\)是一种在客户端和服务器端之间分割模型执行的方法。它可以为整体的通信约束提供不同的选择，但是仍然缺少关于通信值何时显示敏感信息的详细分析。

## 3 提高效率和效益

在本节中，我们将探讨各种技术和开放式问题，以解决使联邦学习更高效和有效的挑战。这包含了无数可能的方法，包括：开发更好的优化算法;为不同的客户端提供不同的模型;使ML任务如超参数搜索、体系结构搜索和调试在FL上下文中更容易;提高通信效率;和更多。

解决这些目标的基本挑战之一是存在非IID数据，因此我们首先调查这个问题并强调潜在的缓解措施。

### 3.1 联邦学习中的非IID数据

虽然IID的含义通常很清楚，但数据可以在很多方面是非IID的。在本节中，我们将提供可能出现于任何客户端分区数据集的非IID数据机制的分类。依赖性和非同一性的最常见来源是由于每个客户端对应于特定用户、特定地理位置和/或特定时间窗口。这种分类法与数据转移的概念有密切的关系\[353,380\]，研究训练分布和测试分布之间的差异;这里，我们考虑每个客户端上数据分布的差异。

以下,考虑一个监督任务特征x和y标签。一个统计模型的联合学习包括两个层次的抽样:访问一个数据需要首先采样一个客户端 i∼Q,客户可用的分布,然后画一个例子$\(x,y\) \sim P\_i\(x,y\)$从客户端的本地数据分布。

当联邦学习中的非IID数据被引用时，这通常指的是$P\_i$和$P\_j$之间的差异，用于不同的客户端i和j。但是，还需要注意的是，分布Q和Pi可能随着时间的变化而变化，这引入了“非IIDness”的另一个维度。

为了完整性，我们注意到，即使考虑到一个单一设备上的数据集，如果数据是在一个不够随机的顺序，例如按时间排序，那么独立性在本地也会被违反。例如，视频中的连续帧是高度相关的。客户端内部关联的来源通常可以通过本地变换来解决。

**非同一客户端分布** 我们首先调查了一些数据偏离恒等分布的常见方式，即对于不同的客户端i和j有$P\_i \neq P\_j$。将Pi\(x, y\)改写为$P\_i\(y \mid x\)P\_i\(x\)$和$P\_i\(x \mid y\)P\_i\(y\)$可以让我们更精确地描述差异：

* **特性分布倾斜\(协变量偏移\)：**即使P\(y \|x\)是共享的，客户端之间的边际分布Pi\(x\)也可能不同。例如，在手写识别领域，用户写同样的单词，可能仍然有不同的笔画宽度，倾斜等。
* **标签分布倾斜\(先验概率偏移\)：**尽管P\(x\|y\)相同，但不同客户端的边际分布Pi\(y\)可能不同。例如，当客户被绑定到特定的地理区域时，标签的分配在不同的客户之间是不同的——袋鼠只存在于澳大利亚或动物园;一个人的脸只在世界上的几个地方;对于移动设备的键盘，某些表情符号只适用于某一人群。
* **相同标签，不同特性\(概念漂移\)：**即使P\(y\)是共享的，条件分布的Pi\(x\|y\)可能在不同的客户端之间是不同的。对于不同的客户，相同的标签y可能有非常不同的特点x，例如，由于文化差异、天气影响、生活水平等。例如，家庭的形象在世界各地可能有很大的不同，衣服的项目也有很大的不同。即使在美国，冬天停车的景象也只会在某些地区被雪覆盖。同样的标签在不同的时间和时间尺度上看起来也会非常不同：白天与夜晚、季节效应、自然灾害、时尚和设计趋势等。
* **相同特性，不同标签\(概念转移\)：**条件分布Pi\(y \|x\)可能在不同的客户端之间不同，即使P\(x\)是相同的。由于个人偏好的原因，同一个训练数据项中的相同特征向量可以有不同的标签。例如，反映情绪或下一个词预测的标签具有个人和地区差异。
* **数量倾斜或不平衡性：**不同的客户端可以持有非常不同的数据量。

现实世界的联邦学习数据集可能包含这些影响的混合物，而在现实世界的分区数据集中描述跨客户端差异是一个重要的开放式问题。大多数关于合成非IID数据集的经验工作\(例如\[337,236\]\)集中于标签分布倾斜，其中非IID数据集是通过基于标签划分一个“扁平的”现有数据集而形成的。更好地理解真实世界的非IID数据集的本质将允许构建受控但真实的非IID数据集，用于测试算法和评估它们对不同程度客户端异构的弹性。

此外，不同的非IID形式可能需要制定不同的缓解战略。例如，在特征分布倾斜的情况下，因为P\(y \|x\)被假定是常见的，所以这个问题至少在原则上是很明确的，并且训练一个单一的全局模型来学习P\(y \|x\)可能是合适的。当相同的特征映射到不同客户的不同标签时，某种形式的个性化\(第3.3节\)可能对学习真正的标签功能是必要的。

**违反独立性** 在训练过程中，一旦分布Q发生变化，就会引入违反独立性的行为;一个突出的例子是跨设备FL，其中设备通常需要满足资格要求才能参加训练\(见第1.1.2节\)。设备通常在当地晚上满足这些要求\(此时它们更有可能在充电、使用免费wi-fi和空闲\)，因此设备的可用性可能有显著的日间模式。此外，由于一天中的当地时间直接对应于经度，这在数据来源中引入了强烈的地理偏差。Eichner等人\[171\]描述了这一问题和一些缓解策略，但仍存在许多悬而未决的问题。

**数据集偏移** 最后，我们注意到分布Q和P的时间依赖性可能会在经典意义上引入数据集偏移\(训练和测试分布之间的差异\)。此外，其他条件可能会使一组符合训练联合模型条件的客户端与将部署该模型的客户端不同。例如，训练可能需要比推理更大的内存设备。这些问题将在第6节中进行更深入的探讨。将处理数据集的技术转变为联邦学习是另一个有趣的开放式问题。

#### 3.1.1 非IID数据的处理策略

联邦学习的最初目标是在客户数据集的联合上训练一个单一的全局模型，但对于非IID数据，这就变得更加困难了。一种自然的方法是修改现有的算法\(例如通过不同的超参数选择\)或开发新的算法，以便更有效地实现这一目标。这种方法将在第3.2.2节中讨论。

对于某些应用程序，可能需要扩充数据，以使客户端之间的数据更加相似。一种方法是创建一个可以全局共享的小数据集。该数据集可能来源于一个公开可用的代理数据源，一个来自客户数据的独立数据集，它对隐私不敏感，或者可能是Wang等人\[473\]之后的原始数据的提纯。

客户目标函数的异构性使如何设计目标函数的问题变得更加重要——不再清楚地认为同等对待所有例子是有意义的。替代方案包括限制来自任何一个用户的数据贡献（这对隐私也很重要，见第4节）和在客户端中引入其他公平的概念;参见第6节的讨论。

但是，如果我们有能力在每个设备上的本地数据上运行训练\(这对于联合学习全局模型是必要的\)，那么训练单个全局模型就是正确的目标吗？在许多情况下，最好使用单一模型，例如，为了向客户提供一个没有数据的模型，或者在部署之前允许手动验证和质量保证。然而，由于本地训练是可能的，每个客户都有一个定制的模型是可行的。这种方法可以将非IID问题从bug转化为特性，几乎是字面上的意思——因为每个客户端都有自己的模型，客户端的身份有效地参数化了模型，使得一些病态但退化的非IID分布变得微不足道。例如，如果对于每个i, Pi\(y\)只支持单个标签，那么找到一个高精度的全局模型可能非常具有挑战性\(特别是如果x是相对无信息的\)，但是训练一个高精度的本地模型是很简单的\(只需要一个常量预测\)。这类多模型方法将在第3.3节中深入讨论。除了解决不相同的客户端分发之外，使用多个模型还可以解决由于客户端可用性变化而导致的独立性冲突。例如，Eichner等人\[171\]的方法使用单一的训练运行，但平均不同的迭代，以便根据客户端的时区/经度提供不同的推理模型。

### 3.2 联邦学习的优化算法

在典型的联邦学习任务中，目标是学习一个单一的全局模型，使整个训练数据集的经验风险函数最小化，也就是跨所有客户端的数据的联合。联邦优化算法和标准分布式训练方法之间的主要区别是需要解决表1的特征——对于优化，非IID和不平衡的数据、有限的通信带宽以及不可靠和有限的设备可用性尤为突出。

在FL方法中，设备的总数是巨大的\(例如跨移动设备\)，使算法必须满足只需要少数客户参与每轮\(客户端采样\)。此外，每个设备在训练一个给定模型时可能只参与一次，所以无状态算法是必要的。这排除了在数据中心上下文中非常有效的各种方法的直接应用，例如ADMM这样的有状态优化算法，以及基于前几轮残留压缩错误修改更新的有状态压缩策略。

联邦学习算法的另一个重要的实际考虑是与其他技术的可组合性。优化算法不是在生产部署中独立运行，而是需要与其他技术相结合，如加密安全聚合协议\(第4.2.1节\)、差分隐私\(DP\)\(第4.2.2节\)和模型和更新压缩\(第3.5节\)。正如在第1.1.2节中提到的，这些技术中的许多都可以应用于原语，如“对选定的客户端求和”和“向选定的客户端广播”，因此根据这些原语表达优化算法提供了有价值的关注点分离，但也可能排除某些技术，如通过异步更新。

最常见的一种优化方法对于联邦学习是联邦平均算法\[337\],一个适应local-update或平行SGD,每个客户端运行在本地的一些SGD步骤,然后更新本地模型平均形成协调服务器上更新全局模型。伪代码在算法1中给出。

算法1：联邦平均\(本地SGD\)，当所有客户端拥有相同数量的数据时。

执行本地更新和减少与中央服务器的通信频率，解决了尊重数据局地约束和移动设备客户端有限的通信能力的核心挑战。然而，从最优化理论的观点来看，这类算法也提出了一些新的算法挑战。在第3.2节中，我们将讨论联邦优化算法在跨客户端分别分发IID和非IID数据的情况下的最新进展和开放式挑战。针对联邦学习设置的特点开发新的算法仍然是一个重要的开放问题。

#### 3.2.1 IID数据集的优化算法和收敛速度

虽然可以对正在优化的每个客户端函数做出各种不同的假设，但最基本的划分是假设IID数据和非IID数据之间的划分。形式上，在客户端上拥有IID数据意味着用于客户端本地更新的每个小批数据在统计上与从整个训练数据集\(客户端上所有本地数据集的联合\)中统一抽取的样本\(有替换\)是相同的。由于客户端独立地收集他们自己的训练数据，这些数据在规模和分布上都不同，而且这些数据不与其他客户或中心节点共享，因此IID假设在实践中显然几乎不成立。然而，这个假设极大地简化了联邦优化算法的理论收敛分析，并建立了一个基线，可以用来理解非IID数据对优化率的影响。因此，第一步自然是了解IID数据用例的优化算法。

形式上，对于IID的设置让我们标准化随机优化问题

$$
\mathop{min}\limits_{x \in R^m}:=\mathop{E}\limits_{z \sim P}[f(x;z)] \nonumber
$$

我们假设一个间歇通信模型，如Woodworth等\[480,第4.4节\]，其中M个无状态客户端参与每T轮，在每轮中，每个客户端可以计算K个样本\(例如小批量\)z1、…， zk采样的IID从P\(可能使用这些采取顺序步骤\)。在IID -数据设置中，客户端是可互换的，我们可以毫不保留地假设M = n。表4总结了本节使用的符号。

对f的不同假设会产生不同的保证。我们将首先讨论凸设置，然后回顾非凸问题的结果。

![image-20210427111207700](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/table-4.png)

表4：讨论包括联邦平均在内的FL算法的表示法。

**凸问题的基准和前沿** 在这一节中，我们回顾了假设随机梯度的方差以$\sigma^2$为界的H-光滑，凸函数\(但不一定是强凸函数\)的收敛结果。更正式地说，H -光滑是指对于所有的z, f\(·;z\)是可微的，并且具有H-Lipschitz梯度，也就是说，对于所有的x, y

$$
|| \nabla f(x,z) - \nabla f(y,z)|| \le H||x-y|| \nonumber
$$

我们还假设所有的x都满足$\nabla _xf\(x;z\)$的随机梯度 $$ {1} \|\| \mathop{E}\limits_{z \sim P}\nabla f\(x,z\) - \nabla F\(x\)\|\| \le \sigma^2\nonumber

$$
当分析具有输出xt在T迭代后的算法收敛速度时，我们考虑这个定义
$$

E\[F\(x\_T\)\]-F\(x^\*\)

$$
当$x^*=arg min_xF(x)。$本文讨论的所有收敛速度都是这一项的上界。表5给出了此类函数的收敛结果总结。

联邦平均(又名并行SGD/本地SGD)与两个自然基线竞争：首先，我们可以在每一轮的本地更新中固定x，并计算当前x处的KM梯度的总数，以运行加速的小批SGD。令$\mathop{x}\limits^-$表示该算法T次迭代的平均值。我们就得到了上界
$$

o\(\frac{H}{T^2}+\frac{\sigma}{\sqrt{TK}}\)\nonumber

$$
比较这两个结果，我们看到，小批SGD获得了最优的“统计”项（$\frac{\sigma}{\sqrt{TKM}}$），而SGD在单个设备上(忽略其他设备的更新)达到最优的“优化”项（$\frac{H}{(TK)^2}$）。

本地更新SGD方法的收敛性分析是一个活跃的研究领域[434,310,500,467,390,371,269,481]。本地更新SGD方法的第一次收敛结果是在强凸的Stich[434]和Y u等人的有非凸目标函数下推导的。这些分析可以达到期望的统计项$\frac{\sigma}{\sqrt{TKM}}$与次优化项(表5中我们总结了这些结果的凸函数的中间)。

![image-20210427115707339](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/table-5.png)

表5：在IID数据方法中一组(不全面的)分布式优化算法的收敛速度。我们假设每次迭代有M个设备参与，损失函数是H-平滑的，凸的，并且我们可以访问方差最多为$σ^2$的随机梯度。在T次迭代之后(可能会有一些迭代平均方案)，所有速率都是(1)的上界。

Wang and Joshi[467]和Stich and Karimireddy[435]通过去除有界梯度假设，可以将优化项进一步提高到HM/T。这些结果表明，如果本地步数K小于$T/M^3$，则(最优)统计项占主导地位。然而，对于典型的跨设备应用程序，我们可能有$T=10^6$和M = 100(表2)，意味着K = 1。

在文献中，收敛界通常伴随着一个关于K可以选择多大的讨论，以便渐进地达到与小批SGD收敛速度相同的统计项。对于强凸函数，Khaled等人[269]改进了这一界，Stich和Karimireddy[435]进一步改进了这一界。

对于非凸目标，Y u等[500]表明，当本地更新次数K小于$T^{1/3}/M$时，本地SGD可以渐近达到误差界$1/\sqrt{TKM}$。Wang和Joshi[467]进一步改进了这种收敛性保证，他们去掉了有界梯度范数假设，并证明本地更新的数量可以大到$T/M^3$。[467]中的分析也可以应用于其他具有本地更新的算法，从而为具有本地更新的分散SGD(或周期性分散SGD)和弹性平均SGD[505]产生第一次收敛保证。Haddadpour等人[216]改进了Wang和Joshi[467]中满足Polyak-Lojasiewicz(PL)函数的界条件[262]，强凸性的推广。特别是hadaddpour等[216]研究表明，对于PL函数，每轮$T^2/M$本地更新会导致O(1/TKM)收敛。

虽然上面的工作关注的是作为执行的迭代数量的函数的收敛，但实践者通常关心的是时钟收敛速度。评估这一点必须考虑到基于通信和本地计算的相对成本的设计参数对每次迭代所花费的时间的影响。从这个角度来看，在保持统计速率的同时关注K可以有多大可能不是联邦学习的主要关注点，在联邦学习中，人们可能会假设几乎是无限的数据集(非常大的N)。增加M的成本(至少在时钟时间上)很小，因此，更自然的做法可能是充分增加M以匹配优化项，然后调整K以最大化时钟优化性能。那么如何选择K呢？在客户端上执行更多的本地更新将增加客户端上产生的本地模型之间的差异，然后才将它们平均。因此，在训练损失方面的误差收敛相对于序列SGD步数T K更慢。然而，执行更多的本地更新可以节省大量的通信成本，并减少每次迭代所花费的时间。最优的本地更新数量在这两种现象之间达到平衡，并实现了与wallclock时间相比最快的误差收敛。Wang和Joshi[468]提出了一种自适应通信策略，该策略根据训练过程中定期间隔的训练损失来调整K。

联邦学习中的另一个重要设计参数是模型聚合方法，该方法用于使用所选客户端所做的更新更新全局模型。在最初的联邦学习论文中，McMahan等人[337]提出根据本地数据集的大小对本地模型进行加权平均。对于IID数据，假设每个客户端都有一个无限大的数据集，这就简化为对本地模型取一个简单的平均值。然而，目前还不清楚这种聚合方法是否会导致最快的误差收敛。

联邦优化中有许多悬而未决的问题，即使是IID数据。Woodworth等人[480]强调了与联邦学习设置相关的优化上界和下界之间的几个差距，特别是“间歇性通信图”，它捕获了局部SGD方法，但这些方法的收敛速度并不知道是否匹配相应的下界。在表5中，我们强调了凸设置的收敛结果。虽然大多数方案能够达到渐近优势统计项，但没有一个方案能够匹配加速小批量SGD的收敛速度。联邦平均算法能否弥补这一差距是一个有待解决的问题。

在本地更新SGD方法中，所有M客户端执行相同数量的本地更新可能会遇到一个常见的可伸缩性问题——如果任何一个客户端不可预测地变慢或失败，它们可能会遇到瓶颈。有几种方法可以解决这一问题，但目前还不清楚哪一种方法是最佳的，特别是考虑到潜在的偏差时(见第6节)。Bonawitz等人[81]建议超额供应客户端(例如，从1.3M个客户端请求更新)，然后接收接收到的第一个M更新，并拒绝来自掉队者的更新。一个稍微复杂一点的解决方案是修复一个时间窗口，并允许客户端在此时间内执行尽可能多的本地更新Ki，在此之后，它们的模型由一个中央服务器平均。Wang等[471]从理论上分析了该方法引入的计算异质性。另一种解决客户端散乱问题的方法是在τ确定局部更新的数量，但允许客户端以异步或无锁定的方式更新全局模型。虽然之前的一些工作[505,306,163]提出了类似的方法，但误差收敛分析是一个开放性和挑战性的问题。然而，在FL设置中一个更大的挑战是，正如第3.2节开头所讨论的，异步方法可能很难与互补的技术相结合，如差分隐私或安全聚合。

除了本地更新的数量外，每个训练轮所选择的客户端集的大小与本地更新的数量之间也存在类似的权衡。在每一轮训练中更新和平均更多的客户端模型可以获得更好的收敛性，但是由于不可预测的计算/与客户端通信的尾部延迟，使得训练容易减速。

非IID条件下的局部SGD / Federated平均化分析更具挑战性;下一节将讨论与此相关的结果和开放问题，以及直接解决非IID问题的专用算法。

### 3.2.2 非IID数据集的优化算法和收敛速度

与集中式学习中由独立和相同分布(IID)示例组成的有序小批量不同的是，联邦学习使用来自最终用户设备的本地数据，导致多种非IID数据(第3.1节)。

在这种情况下，每个N个客户端都有一个本地数据分布pi和一个本地目标函数
$$

f_i\(x\) = \mathop{E}\limits_{z \sim P\_i}\[f\(x;z\)\]\nonumber

$$
我们记f(x;z)是模型x在例子z处的损失。我们通常希望最小化
$$

F\(x\)=\frac{1}{N}\sum^N\_1f\_i\(x\)

$$
注意，当每个Pi相同时，我们恢复IID方法，我们让$F^*$表示F的最小值，得到点$x^*$。类似地，我们将让$f_i^*$*表示fi的最小值。

在IID方法中，我们假设一个间歇通信模型(例如Woodworth等人[480,sec4.4])，其中M个无状态客户端参与每T轮，在每轮中，每个客户端可以为K个样本(例如小批量)计算梯度。不同的是样本$z_{i,1},...,z_{i,K}$在客户端i采样，从客户端局部分布Pi中抽取。与IID方法不同，我们不能假定M = N，因为客户端分布并不都是相等的。在下面，如果一个算法依赖于M = N，我们将省略M而简单地写N。我们注意到，虽然这样的假设可能与表1中的跨竖井联邦设置兼容，但在跨设备设置中通常是不可行的。

虽然(434、500、467、435)主要集中在IID情况，分析技术可以扩展到非IID案例通过添加一个假设的数据,例如通过约束客户梯度和全球梯度(305、300、304、469、471)之间的区别或客户端和全局最优值的区别(303年268]。在此假设下，Yu等[501]表明，在非IID情况下，局部SGD的误差界变差。为了达到$1/\sqrt{TKN}$(非凸目标下)的速率，局部更新次数K应小于$T^{1/3}/N$，而不是IID情况下的$T/N^3$[467]。Li等人[300]提出在每个局部目标函数中增加近端项，使算法对本地目标间的异构性更加鲁棒。提出的FedProx算法提高了联邦平均的性能。Khaled等人[268]假设所有客户都参与，并对客户使用批量梯度下降，这可能比客户端的随机梯度收敛得更快。

最近，为了更好地应用于联邦平均法的实际应用，许多研究在放宽分析所必需的假设方面取得了进展。例如，Li等人[303]在更现实的情况下研究了Federated average的收敛性，在这种情况下，每一轮中只有一部分客户端参与。为了保证收敛，他们假设客户要么是均匀随机选择的，要么是与本地数据集的大小成比例的概率选择的。尽管如此，在实践中，服务器可能无法以这些理想化的方式对客户机进行抽样——特别是仅在跨设备设置中，满足严格资格要求(如充电、空闲、免费WiFi)的设备将被选择参与计算。在一天的不同时间，客户的特征可能会有很大的差异。Eichner等人[171]提出了这个问题，并研究了半循环SGD的收敛性，即按照一个规律的循环模式(例如，日循环)采样多个具有不同特征的客户块。由于计算能力的异构性，客户端可以执行不同的本地步骤。Wang等[471]证明了FedAvg等许多联邦学习算法在存在异构局部步长时能收敛到不匹配目标函数的静止点。他们将这个问题称为客观不一致性，并提出了一种简单的技术来消除联邦学习算法的不一致性问题。

我们在表6中总结了最近的理论结果。表6中所有的方法对客户端上的局部函数都假定平滑或Lipschitz梯度。凸函数的误差界由最优目标(1)和非凸函数的梯度范数来度量。对于每种方法，我们给出了关键的非iid假设、对每个客户端函数fi(x)的假设以及其他辅助假设。我们也将每种方法简单地描述为联邦平均算法的变体，并展示了简化的收敛速度消除常数。假设客户端函数是强凸的，可以加快收敛速度[303,265]。有界梯度方差是分析随机梯度方法中广泛使用的假设，在客户使用随机本地更新时经常使用[305,303,304,499,265]。Li等[303]直接分析了联邦平均算法，该算法每轮对随机抽样的M个客户端使用K步本地更新，并提出了本地更新(K > 1)会减慢收敛速度的速率。阐明K > 1对收敛的影响是一个重要的开放性问题。

![image-20210427153822806](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/table-6.png)

表6：在非iid设置中一组(不全面的)联邦优化方法的收敛速度。我们总结了非iid数据的关键假设、每个客户机上的本地函数以及其他假设。我们还提出了与联邦平均算法和消除常数的收敛速度相比的算法的变体。

**连接到分散优化** 联邦优化的目标函数在分散优化界已经研究了多年。Wang和Joshi[467]首先指出，在适当设置网络拓扑矩阵(混合矩阵)的情况下，分散SGD的收敛性分析可以应用于或与局部SGD结合。为了减少通信开销，Wang和Joshi[467]提出了周期性分散SGD (PD-SGD)，允许分散SGD有多个本地更新，称为联邦平均。Li等人[304]将该算法扩展到非iid情况。MATCHA[469]通过随机抽样客户端进行计算和通信，进一步提高了PD-SGD的性能，并提供了一种收敛性分析，表明本地更新可以加速收敛。

**加速度，方差减少和自适应** 动量、方差减少和自适应学习率都是改善一阶方法收敛性和泛化的有前途的技术。但是，没有一种单一的方式将这些技术纳入FedAvg。SCAFFOLD[265]使用控制变量对客户端更新的差异进行建模，以执行方差减少。值得注意的是，这允许收敛结果不依赖于限制客户机之间的异构量。对于动量，Yu等人[501]提出允许每个客户端保持一个局部动量缓冲区，并在每一轮通信中平均本地缓冲区和本地模型参数。虽然这种方法提高了本地SGD的最终精度，但这使每轮通信成本增加了一倍。Xie等人使用了类似的方案[485]设计了本地SGD的变体，其中客户端在本地执行Adagrad[335, 161]。Reddi等人[389]提出在服务器级使用自适应学习率，开发与FedAvg通信成本相同的自适应优化方法联邦版本。该框架是Hsu等人[237]、Wang等人[470]提出的服务器动量框架的总结，该框架允许动量而不增加通信成本。虽然两者[501,470]都表明局部SGD的动量变量能够以与同步小批SGD相同的速率收敛到非凸目标函数的平稳点，但在联邦学习设置中证明动量加速收敛速度是具有挑战性的。最近Karimireddy等人[264]提出了一种将集中式优化算法适应异构联邦设置的通用方法(MIME框架和算法)

## 3.3 多任务学习、个性化和元学习

在本节中，我们将考虑各种“多模型”方法——这些技术可以在推断时有效地为不同的客户机使用不同的模型。这些技术在面对非iid数据时尤其相关(第3.1节)，因为它们的性能甚至可能超过最佳的共享全局模型。我们注意到，个性化也在完全分散的设置中进行了研究[459,59,504,19]，其中训练个体模型是特别自然的。

## 3.3.1 通过特征实现个性化

本节的其余部分将专门考虑使用不同的模型参数（权重）导致不同用户运行推理的技术。然而，在一些应用程序中，通过简单地向模型添加用户和上下文特性也可以获得类似的好处。例如，考虑Hard等人[222]所述的移动键盘下一个单词预测的语言模型。不同的客户机可能会以不同的方式使用语言，事实上，对模型参数进行设备上的个性化已经为这个问题带来了显著的改进[472]。然而，一个互补的方法可能是训练一个联邦模型，它不仅将用户迄今为止输入的单词作为输入，还将其他各种用户和上下文特性作为输入——该用户经常使用什么单词？他们目前使用的是什么应用程序？如果他们在聊天，他们之前给这个人发过什么信息？通过适当的特性，这样的输入可以允许共享的全局模型产生高度个性化的预测。然而，很大程度上由于很少有公共数据集包含这样的辅助功能，开发能够有效地为不同任务合并上下文信息的模型体系结构仍然是一个重要的开放问题，有可能极大地提高FL训练模型的效用。

### 3.3.2 多任务学习

如果将每个客户端的本地问题(本地数据集上的学习问题)视为一个单独的任务(而不是单个分区数据集的碎片)，那么多任务学习技术[506]立即变得相关。值得注意的是，Smith等人[424]引入了用于多任务联邦学习的MOCHA算法，直接解决了通信效率、掉线者和容错的挑战。在多任务学习中，训练过程的结果是每个任务一个模型。因此，大多数多任务学习算法假设所有的客户端(任务)都参与了每个训练轮，并且也需要有状态的客户端，因为每个客户端都在训练一个单独的模型。这使得这些技术与跨竖井FL应用程序相关，但更难应用于跨设备场景。

另一种方法是重新考虑客户端（本地数据集）和学习任务（需要训练的模型）之间的关系，观察单个全局模型和每个客户端不同模型之间的关键点。例如,它可能会从多任务学习应用技术(以及其他方法如个性化,接下来讨论),我们把“任务”的一个子集的客户,也许选择明确(如基于地理区域或特征的设备或用户),或者可能基于集群[331]或客户端上已学习图的连接组件[504]。这类算法的发展是一个重要的开放性问题。参见第4.4.4节，讨论了如何处理稀疏联邦学习问题，比如在这种类型的多任务问题中自然出现的问题，而不透露每个客户端属于哪个客户端子集(任务)。

### 3.3.3 本地微调和元学习

通过本地微调，我们指的是一些技术，这些技术从单个模型的联邦训练开始，然后部署为所有客户端建模，在使用推论之前通过在本地数据集上附加训练被特征化。这种方法自然地集成到联邦学习模型的典型生命周期中(第1.1.1节)。对全局模型的训练仍然可以在每一轮中只使用一小部分客户样本(例如100个);当模型被部署时，将全局模型广播给所有客户端(例如数百万)只发生一次。唯一的区别是，在使用模型对客户端进行实时预测之前，会进行最后的训练过程，根据本地数据集对模型进行个性化处理。

给定一个表现相当不错的全球模型，使其个性化的最佳方法是什么？在非联邦学习中，研究人员经常使用微调、迁移学习、领域适应[329,132,61,332,133]，或基于个人局部模型的插值。当然，用于这种插值的精确技术是关键，在联邦学习的环境中确定其相应的学习保证是很重要的。此外，这些技术通常只假设一对领域(源和目标)，因此联邦学习的一些更丰富的结构可能会丢失。

研究个性化和非iid数据的一种方法是通过连接元学习，元学习已经成为一种流行的模型适应环境。在标准的学习-学习(LTL)设置[56]中，有一个任务的元分布，样本用于学习学习算法，例如通过找到假设空间的良好限制。这实际上与第3.1节中讨论的统计设置非常匹配，其中我们对客户机(任务)i∼Q进行抽样，然后从Pi中对该客户机(任务)的数据进行抽样。

最近，一类被称为模型无关元学习(MAML)的算法被开发出来，元学习是一个全局模型，它可以作为学习一个适合于给定任务的良好模型的起点，只使用几个本地梯度步骤[187]。最值得注意的是，流行的爬虫算法[358]的训练阶段与联邦平均[337]密切相关——爬虫允许服务器学习率，并假设所有客户端拥有相同的数据量，但其他方面是相同的。Khodak等人[270]和江等人[250]探索FL和MAML之间的联系,并说明MAML设置相关框架模型的个性化目标FL。FL. Chai Sim等人[102]地方微调个性化语音识别模型应用于联合学习。Fallah等人[181]通过将MAML而不是爬虫连接到联邦学习，开发了一种名为个性化FedAvg的新算法。在[299]中研究了其他具有差分隐私的连接。

将FL和MAML的思想结合起来的大致方向是比较新的，有许多开放性的问题：

- 对监督任务的MAML算法的评价主要集中在合成图像分类问题上[290,386]，其中无限的人工任务可以通过对图像类进行子采样来构建。FL问题由现有的用于模拟FL实验的数据集建模(附录A)，可以作为MAML算法的现实基准问题。
- 除了实证研究或优化结果之外，分析MAML类型技术的理论保证并研究在哪些假设下它们可以成功是有用的，因为这将进一步阐明它们可能适用的FL域集合。
- 观察到的全局和个性化准确性之间的差距[250]创造了一个很好的论点，个性化应该是FL的核心重点。然而，现有的工作中没有一个明确地制定出衡量个性化表现的全面指标;例如，对每个客户端进行小的改进是否比对一部分客户端进行大的改进更可取？有关讨论请参见第6节。
- Jiang等人[250]强调了一个事实，即相同结构和性能的模型，但经过不同的训练，在个性化方面的能力可能非常不同。特别是，似乎以最大化全局性能为目标的训练模型实际上可能会损害模型后续个性化的能力。理解这一现象的潜在原因是一个与FL和更广泛的ML社区有关的问题。
- 在这个多任务/LTL框架中，包括个性化和隐私在内的几个具有挑战性的FL主题已经开始被研究[270,250,299]。其他问题，如概念漂移，是否也可以用这种方法来分析，例如作为终身学习的一个问题[420]？
- 非参数传输LTL算法，如ProtoNets[425]，可以用于FL吗？

### 3.3.4 什么时候全局联邦训练模型更好？

什么是联邦学习可以为你做的而本地训练不能在一个设备上？当局部数据集很小且数据为IID时，FL显然具有优势，实际上，联邦学习的真实应用[491,222,112]受益于跨设备训练单一模型。另一方面，给定病态的非iid分布(例如Pi(y |x)直接不同于客户端)，本地模型将做得更好。因此，一个自然的理论问题是确定在什么条件下共享全局模型比独立的每个设备模型更好。假设我们为每个客户k训练一个模型$h_k$，使用从该客户端获得的大小为$m_k$的样本。对于客户k，我们能否保证通过联邦学习学习到的模型$h_k$至少和$h_{FL}$一样准确？我们能否量化通过联邦学习可以预期得到多少改进？我们能否在理论上保证至少匹配两个自然基线($h_k$和$h_{FL}$)的性能的情况下开发个性化策略？

其中一些问题与先前关于多源适应和不可知论联邦学习的工作有关[329,330,234,352]。这些问题的难度取决于数据如何在各方之间分布。例如，如果数据是垂直分区的，那么各方维护关于公共实体的不同特性集的私有记录，这些问题可能需要在联邦学习任务中寻址记录链接[124]。独立于私下进行录音连接的最终技术征税之外[407]，这项任务本身在现实世界中是非常容易产生噪音的[406]，只有稀疏的结果解决了它对训练模型的影响[224]。健壮性和隐私技术可以使本地模型相对更强，特别是对于非典型客户机[502]。损失分解技巧可用于监督学习，以减轻垂直划分假设本身，但实际效益取决于数据的分布和参与者的数量[373]。

## 3.4 为联邦学习适应ML工作流

当将标准机器学习工作流和流水线(包括数据增强、特征工程、神经体系结构设计、模型选择、超参数优化和调试)适应于分散的数据集和资源受限的移动设备时，会出现许多挑战。我们将在下面讨论其中的几个挑战。

### 3.4.1 超参数调优

在资源受限的移动设备上使用不同的超参数运行多轮训练可能会受到限制。对于小的设备群，这可能会导致过度使用有限的通信和计算资源。然而，最近的深度神经网络关键依赖于关于神经网络的结构、正则化和优化的广泛的超参数选择。对于大型模型和大型设备上的数据集，计算可能是昂贵的。超参数优化 (HPO)在AutoML框架下有着悠久的历史[395,273,277]，但它主要关注的是如何提高模型的准确性[64,426,374,180]，而不是移动设备的通信和计算效能。因此，我们希望进一步的研究应该考虑在联邦学习的背景下开发高效的超参数优化解决方案。

除了超参数优化问题的通用方法外，在训练领域，特别是易于调优的优化算法的开发是一个主要的开放领域。集中训练已经需要优化参数，如学习率、动量、批大小和正则化。联邦学习可能会添加更多的超参数——聚合/全局模型更新规则和本地客户端优化器的单独调优、每轮选择的客户端数量、每轮本地步骤的数量、更新压缩算法的配置等等。这些超参数对于在精度和收敛性之间取得良好的平衡是至关重要的，并且可能实际上影响学习模型的质量[106]。除了高维搜索空间外，联邦学习通常还需要更长的时钟训练时间和有限的计算资源。这些挑战可以通过对超参数设置具有鲁棒性的优化算法(相同的超参数值适用于许多不同的现实世界数据集和体系结构)，以及自适应或自调优算法来解决[446,82]。

### 3.4.2 神经结构设计

联邦学习方法的神经架构搜索(NAS)的动机是目前应用预定义深度学习模型的实践的缺点:当用户生成的数据对模型开发人员不可见时，深度学习模型的预定义架构可能不是最优设计选择。例如，对于特定的数据集，神经体系结构可能有一些冗余组件，这可能导致设备上不必要的计算;对于非IID数据分布可能有更好的体系结构设计。第3.3节中讨论的个性化方法仍然在所有客户机之间共享相同的模型体系结构。NAS的最新进展[230,387,175,388,60,375,313,488,175,323]为解决这些缺陷提供了一种潜在的方法。NAS主要有三种方法，利用进化算法、强化学习或梯度下降来搜索特定数据集上特定任务的最优架构。其中，基于梯度的方法利用高效的梯度反向传播和权重共享，将架构搜索过程从3000多GPU天减少到1 GPU天。最近发表的另一篇有趣的论文，涉及权重不可知的神经网络[192]，声称无需学习任何权重参数的神经网络体系结构，就可以为给定的任务编码解决方案。如果该技术进一步发展并得到广泛应用，可以应用于无需设备间协作训练的联邦学习。虽然这些方法还没有为联邦学习等分布式方法开发，但它们都可以被转移到联邦方法中。联邦学习方法中用于全局或个性化模型的神经结构搜索(NAS)很有前途，在[228]中已经进行了早期探索。

### 3.4.3 FL的调试和解释性

虽然在模型的联合训练方面已经取得了实质性的进展，但这只是完整的ML工作流程的一部分。有经验的建模者经常直接检查数据的子集，包括基本的完整性检查，调试错误分类，发现异常值，手动标记示例，或检测训练集中的偏差。开发隐私保护技术来回答去中心化数据上的这类问题是一个主要的开放性问题。最近，Augenstein等人[31]提出了使用经过联邦学习训练的差分隐私生成模型(包括GANs)来回答这类问题。然而，仍有许多悬而未决的问题(参见[31]中的讨论)，特别是提高FL DP生成模型保真度的算法的发展。

## 3.5 通信与压缩

现在大家都很清楚，通信可能是联邦学习的主要瓶颈，因为无线链路和其他终端用户互联网连接的速率通常低于数据中心内或数据中心间的链路，而且可能昂贵和不可靠。这导致了最近人们对减少联邦学习的通信带宽的极大兴趣。将联邦平均与模型更新的稀疏化和/或量化结合到少量位的方法已经证明了在对训练精度影响最小的情况下显著降低了通信成本[282]。然而，目前还不清楚是否可以进一步降低通信成本，以及这些方法或它们的组合是否可以在联邦学习中提供通信和准确性之间的最佳权衡。在理论统计学中，对准确性和沟通之间的这种基本权衡的特征化是最近的兴趣[507,89,221,7,49,444,50]。这些工作描述了在通信约束下的分布式统计估计和学习的最优最小最大速率。然而，由于这些理论工作通常忽略了优化算法的影响，很难从实践中推导出具体的见解。利用这种统计方法为实际训练方法提供信息，仍然是一个开放的方向。

**压缩目标** 由于当前设备在计算、内存和通信方面的资源有限，因此有几个具有实用价值的不同压缩目标。

1. 梯度压缩 -减少从客户端到服务器的通信对象的大小，用于更新全局模型。
2. 模型广播压缩-减少从服务器到客户端的模型广播的大小，客户端从这里开始本地训练。
3. 局部计算减少-对整体训练算法的任何修改，使局部训练过程在计算上更有效率。

这些目标在大多数情况下是互补的。其中，就总运行时间而言，（1）有可能产生最显著的实际影响。这一方面是因为客户端连接的上传带宽通常比下载带宽慢——因此与(2)相比，可以获得更多的带宽——另一方面是因为在多个客户端上平均的效果可以实现更激进的有损压缩方案。(3）通常可以与(1)、(2)通过具体方法共同实现。

许多现有文献适用于(1)目标[282,440,281,17,235,55]。直到最近才研究(2)对总体趋同的影响;在[123]中进行了分析。很少有方法打算联合处理所有(1)、(2)和(3)。Caldas等人[95]提出了一种实用的方法，通过约束期望的模型更新，使客户端只需要模型变量的特定子矩阵;Hamer等人[219]提出了一种有效通信的联邦算法，在预先训练的模型集合上学习混合权值，该算法的基础知识是任意一个设备通信模型的子集;He等人[227]利用双向和可选的知识蒸馏方法将知识从许多紧凑的DNN转移到一个密集的服务器DNN，这可以减少边缘设备的局部计算负担。

在跨设备FL中，算法通常不能假设客户端上保留任何状态(表1)。然而，在跨竖井FL设置中，这种约束通常不会出现，因为相同的客户端会重复参与。因此,更广泛的观点与纠错等(311,405,463,444,263,435]在此设置有关,其中许多可以解决(1)和(2)。

另外一个目标是修改的训练过程,最终的模型更紧凑,或为推理效率。这个主题在更广泛的ML社区中得到了大量的关注[220,138,509,309,362,74]，但是这些方法要么没有一个直接的联邦学习映射，要么使训练过程更加复杂，从而难以采用。研究同时产生一个紧凑的最终模型，同时也解决了上述三个目标，具有重大的实际影响的潜力。

对于梯度压缩，现有的一些工作[440]是在极大极小意义上发展的，以描述最坏的情况。然而，通常在信息论中，压缩保证是特定于实例的，并依赖于底层分布的熵[140]。换句话说，如果数据很容易压缩，那么它们就会被严重压缩。这将是有趣的，看看是否类似的实例具体结果可以得到梯度压缩。类似地，最近的研究表明，在数据压缩和梯度压缩情况下，以依赖数据的方式学习压缩方案可以显著提高压缩比[482]。因此，有必要评估联邦设置中的这些依赖于数据的压缩方案[193]。

**不同的隐私和安全聚合的兼容性** 联邦学习中使用的许多算法，如安全聚合[79]和添加噪声以实现差分隐私的机制[3,338]，都不是为压缩或量化通信而设计的。例如，Bonawitz等人[80]、Bell等人[58]的安全聚合协议的直接应用需要为每个标量增加O(logM)位的通信，其中M是被求和的客户端数量，当M很大时，这可能导致更新的积极量化无效(尽管见[82]以获得更有效的方法)。现有的噪声添加机制假设在每个客户端上添加实值高斯或拉普拉斯噪声，这与用于减少通信的标准量化方法不兼容。我们注意到最近的一些工作允许有偏差的估计，并且可以很好地处理拉普拉斯噪声[435]，但是这些不会提供不同的隐私，因为它们打破了回合之间的独立性。已经有一些关于添加离散噪声[9]的工作，但是还没有关于这些方法是否最优的概念。因此，联合设计与安全聚合兼容的压缩方法，或者可以获得不同的隐私保证，是一个有价值的开放问题。

**无线FL协同设计** 

现有的联邦学习文献通常忽略了模型训练过程中无线信道动态的影响，这可能会降低训练延迟，从而降低整个生产系统的可靠性。特别是，无线干扰、噪声信道和信道波动会显著阻碍服务器和客户机之间的信息交换(或直接阻碍单个客户机之间的信息交换，如完全分散的情况，见第2.1节)。这是关键任务应用程序的主要挑战，其根源在于减少延迟和增强可靠性。解决这一挑战的潜在解决方案包括联邦蒸馏(FD)，其中工作者交换他们的模型输出参数(logit)而不是模型参数(梯度和/权重)，并通过适当的通信和计算资源优化工作者的调度策略[248,368,402]。另一种解决方案是利用无线信道的独特特性(例如广播和叠加)作为自然数据聚合器，不同工作者同时传输的模拟波在服务器上叠加，并通过无线信道系数[4]进行加权。这在服务器上产生了更快的模型聚合，以及更快的训练速度，最多可以增加工人的数量。这与传统的正交频分复用(OFDM)模式形成了鲜明的对比，在传统的OFDM模式下，工作者在正交频率上上传模型，而正交频率的性能随着工作者数量的增加而下降[174]。

## 3.6 应用于更多类型的机器学习问题和模型

到目前为止，联邦学习主要考虑的是监督学习任务，其中标签在每个客户机上自然可用。将FL扩展到其他ML范式，包括强化学习、半监督和非监督学习、主动学习和在线学习[226,508]，都呈现出有趣和开放的挑战。

另一类与FL高度相关的重要模型是那些可以描述其预测中的不确定性的模型。大多数现代深度学习模型不能代表它们的不确定性，也不允许参数学习的概率解释。这推动了将贝叶斯模型与深度学习相结合的工具和技术的最新发展。从概率论的角度来看，使用单点估计进行分类是不合理的。贝叶斯神经网络[419]已经被提出并被证明对过拟合具有更强的鲁棒性，并且可以很容易地从小数据集中学习。贝叶斯方法通过其参数的概率分布形式进一步提供了不确定性估计，从而防止了过拟合。此外，借助概率推理，人们可以预测不确定性如何减少，随着数据量的增加，网络做出的决策变得更加确定性。

由于贝叶斯方法为深度模型的可信度提供了新的储备，并在许多任务上取得了最先进的性能，人们期望贝叶斯方法为经典的联邦学习提供一个概念上的改进。事实上，Lalitha等人的初步工作[292]表明，结合贝叶斯方法可以在非iiddata和异构平台上进行模型聚合。然而，许多关于可伸缩性和计算可行性的问题必须得到解决。

## 3.7 主要总结

与在数据中心进行集中训练相比，高效有效的联邦学习算法面临着不同的挑战。

- 由于不相同的客户端分布、违反独立性和数据集漂移(第3.1节)而产生的非iid数据构成了一个关键挑战。虽然本节调查和讨论了各种方法，但定义和处理非iid数据仍然是一个开放性问题，也是联邦学习中最活跃的研究主题之一。
- 在第3.2节中分析了联邦学习在凸函数和非凸函数、IID和非IID数据等不同设置下的优化算法。对于联邦优化中常用的并行本地更新，理论分析已被证明是困难的，并且通常必须做出严格的假设来约束客户机的异构性。目前，已知的收敛速度并不能完全解释联邦平均算法在小批量SGD等方法上的经验观察效果[481]。
- 客户端个性化和“多模型”方法(第3.3节)可以解决数据异构问题，并有望超越最佳固定全局模型的性能。像微调这样简单的个性化方法是有效的，并提供内在的隐私优势。然而，许多理论和实证问题仍然悬而未决：什么时候全局模型更好？需要多少模型？哪个联邦优化算法与本地微调结合得最好?
- 将集中训练工作流程(如超参数调优、神经结构设计、调试和可解释性任务)适应联邦学习设置(第3.4节)是FL在实际设置中广泛采用的障碍，因此构成了重要的开放式问题。
- 虽然在FL的通信效率和压缩方面已经有了大量的研究(第3.5节)，但它仍然是一个重要和活跃的领域。特别是，在不影响广泛类型模型收敛的情况下实现压缩过程的完全自动化是一个重要的实际目标。讨论了通信理论研究的新方向、与隐私方法的兼容性以及与无线基础设施的协同设计。
- 将联邦学习从监督任务扩展到其他机器学习范式，包括强化学习、半监督和非监督学习、主动学习和在线学习，还有许多有待解决的问题(第3.6节)。

# 4 保护用户数据隐私

机器学习的工作流程涉及许多不同能力的参与者。例如，用户可以通过与他们的设备交互产生训练数据，机器学习训练过程从这些数据中提取跨人群模式(例如以训练模型参数的形式)，机器学习工程师或分析师可以评估训练模型的质量，最终，模型可能被部署到最终用户，以支持特定的用户体验(参见下面的图1)。

![](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/figure-1.png)

图1：经过fl训练的模型的生命周期和联邦学习系统中的各种参与者。(重复之前的)

在理想情况下，系统中的每一个参与者只会学到发挥其作用所需的信息。例如，如果分析师只需要确定某个特定的质量度量是否超过了期望的阈值，以便授权向终端用户部署模型，那么在理想情况下，这是分析师能够获得的唯一信息;例如，这样分析师既不需要访问训练数据，也不需要访问模型参数。类似地，终端用户享受由训练过的模型提供的用户体验，可能只需要模型的预测，而不需要其他任何东西。

此外，在理想的情况下，该系统的每一个参与者都能够轻松准确地推断，他们参与该系统可能会透露自己和他人的哪些个人信息，参与者将能够利用这一理解，做出明智的选择，关于如何参与和是否参与。

生产系统与上述理想隐私属性就其本身而言将是一个艰巨的壮举,更是如此,同时保证其他可取的属性,如易用性为所有参与者,终端用户体验的质量和公平(权力)的模型,明智地使用通信和计算资源，抵御攻击和失败的弹性，等等。

我们不允许完美成为良好的敌人，我们提倡这样一种策略，即整个系统由可以相对独立地研究和改进的模块单元组成，同时也提醒自己，我们最后必须，用我们上述理想的隐私目标来衡量完整系统的隐私属性。本节中提出的开放式问题将突出一些领域，在这些领域，我们还不知道如何同时实现我们的所有目标，无论是为单个模块还是为整个系统。

联邦学习提供了一个有吸引力的结构，将整个机器学习工作流分解成我们想要的可接近的模块单元。联邦学习模型的主要吸引力之一是，它可以通过最小化数据为参与的用户提供一定程度的隐私：原始用户数据永远不会离开设备，只有对模型的更新(例如，梯度更新)被发送到中央服务器。与原始数据相比，这些模型更新更关注于手头的学习任务(即，它们严格不包含关于用户的额外信息，与原始数据相比，通常要少得多)，并且单个更新只需要由服务器临时保存。

虽然这些特性可以在集中所有训练数据的基础上提供显著的实际隐私改进，但在这个基线联邦学习模型中仍然没有正式的隐私保证。例如,可以构造场景的原始数据信息泄露从客户机到服务器,如一个场景,知道前面的模型和梯度更新从用户将允许用户持有的推断出一个训练的一个例子。因此，本节将调查现有的结果，并概述在设计能够提供严格隐私保证的联邦学习系统方面面临的开放式挑战。我们将重点放在联邦学习和分析设置中特定的问题上，而将[344]中调查的更一般的机器学习设置中也会出现的问题放在一起。

除了针对用户隐私的攻击，还有其他种类的针对联邦学习的攻击;例如，对手可能试图阻止一个模型完全被学习，或者他们可能试图对模型产生偏见，以产生比对手更好的推论。我们将对这类攻击的考虑推迟到第5节。

本节的其余部分组织如下。第4.1节讨论了我们希望提供保护的各种威胁模型。第4.2节列出了一套核心工具和技术，可用于提供针对第4.1节中讨论的威胁模型的严格保护。第4.3节假设了可信服务器的存在，并讨论了在提供针对对抗性客户端和/或分析师的保护方面存在的开放问题和挑战。第4.4节讨论了在没有完全可信的服务器的情况下出现的问题和挑战。最后，第4.5节讨论了关于用户感知的开放性问题。

## 4.1 参与者、威胁模型和深度隐私

FL的隐私风险的正式处理需要一个整体和跨学科的方法。虽然一些风险可以映射到技术隐私定义，并通过现有技术减轻，但其他风险更复杂，需要跨学科的努力。

隐私不是一个二进制量，甚至不是一个标量。实现这种正式处理的第一步是仔细描述不同的参与者(见第1节的图1，为了方便起见，在上面重复)及其角色，从而最终定义相关的威胁模型(见表7)。需要区分服务器管理员的观点和分析师的观点,消费学模型,可想而知,一个系统,旨在提供强大的隐私担保对恶意分析师可能不提供任何担保对恶意服务器。这些参与者很好地反映了文献中其他地方讨论过的威胁模型;例如，在Bittau等人[73，第3.1节]中，“编码器”对应于客户端，“shuffle”一般对应于服务器，“分析器”可能对应于服务器或由分析师完成的后处理。

![image-20210429100239124](https://gitee.com/zlt_shadow/res/raw/master/images-bed/paper4/table-7.png)

表7：针对不同敌对行为者的各种威胁模型。

例如，一个特定的系统可以向服务器管理员提供一个带有特定参数ε的差异化隐私保证，而分析师观察到的结果可能具有更高的保护$\varepsilon^{'} > \varepsilon$。

此外，这种保证可能只适用于能力受到特别限制的对手，例如，一个对手可以观察服务器上发生的一切(但不能影响服务器的行为)，同时控制客户端(观察他们看到的一切，并以任意的方式影响他们的行为)的一小部分$\gamma$;对手也可能被认为无法破坏在特定安全级别σ下实例化的密码机制。对于实力超过这些限制的对手，服务器管理员的视图可能仍然有一些不同的隐私，但处于较弱的级别$\varepsilon_{0} > \varepsilon$。

正如我们在这个例子中看到的，精确地指定一个系统的假设和隐私目标可以很容易地涉及几个参数(ε， $\varepsilon^{'}$， $\varepsilon_{0}$， γ， σ等)的具体实例，以及诸如差分隐私和诚实但好奇的安全等概念。

要实现联邦学习所需的所有隐私属性，通常需要将下面描述的许多工具和技术组合成一个端到端系统，可能两个分层多种策略来保护相同的系统的一部分(例如在可信执行环境(TEE)中运行安全多方计算(MPC)协议的一部分，以使对手更难充分破坏该组件)以及保护的不同部分使用不同的策略(例如，使用MPC保护模型更新的聚合，然后使用Private Disclosure技术，然后在服务器之外共享聚合更新)。

因此，我们主张构建联邦系统，在其中，当一种或另一种技术未能提供其预期的隐私贡献时，隐私属性会尽可能优雅地退化。例如，在TEE中运行MPC协议的服务器组件可能允许在TEE安全性或MPC安全性假设中任何一个(但不是两个)在实践中失败的情况下维护隐私。另一个例子是，比起让客户端向服务器端TEE发送渐变更新，要求客户端向服务器端TEE发送原始训练示例是非常不可取的，因为如果TEE的安全性失败，后者的隐私期望会更优雅地降低。我们将这种优雅退化的原则称为“深度隐私”，类似于成熟的深度防御网络安全原则[361]。

## 4.2 工具和技术准备

一般来说,一个FL计算的目标是分析师或工程师的要求计算得到的结果,可以认为是评价函数f的一个分布式客户端数据集(通常一个ML模型训练算法,但可能更简单的东西,比如基本统计)。有三个隐私方面需要解决。

首先，我们需要考虑f是如何计算的，以及过程中中间结果的信息流是什么，这主要影响对恶意客户机、服务器和管理参与者的敏感性。除了设计系统中的信息流(例如早期数据最小化)，来自安全计算的技术，包括安全多方计算(MPC)和可信执行环境(tee)，与解决这些问题特别相关。这些技术将在第4.2.1节中详细讨论。

其次，我们必须考虑计算了什么。换句话说，f自身的结果向分析师和世界参与者透露了多少关于参与客户的信息。在这里，隐私保护披露的技术，特别是差分隐私(DP)，是高度相关的，将在第4.2.2节详细讨论。

最后，还有一个可验证性问题，它涉及到客户端或服务器向系统中的其他人证明他们已经忠实地执行了期望的行为，而不暴露他们所依据的潜在的私有数据。可验证性的技术，包括远程认证和零知识证明，将在第4.2.3节讨论。

### 4.2.1 安全计算

安全计算的目标是计算分布式输入上的函数，其方式是只向预期各方显示计算结果，而不显示任何附加信息(例如各方的输入或任何中间结果)。

**安全多方计算** 安全多方计算(Secure multi-party computation, MPC)是密码学的一个子领域，它涉及的问题是让一组参与者计算其私有输入的约定函数，而该函数只向每一方显示预期的输出。这一领域在1980年代由Yao[493]开创。由于理论和工程上的突破，该领域已经从纯粹的理论兴趣转变为工业上的部署技术[78,77,295,27,191,242,243]。值得注意的是，MPC定义了一组技术，应该更多地被视为安全计算中的一个领域或一个安全的一般概念，而不是一种技术本身。MPC的一些最新进展可以归因于低层原语的突破，如遗忘传输协议[244]和具有同态特性的加密方案(如下所述)。

密码解决方案的一个常见方面是，运算通常是在有限域上完成的(例如，用素数p取模的整数)，这在表示实数时造成了困难。一种常见的方法是调整ML模型及其训练过程，以确保(过)下流得到控制，方法是对标准化数量进行操作，并依赖谨慎的量化[194,10,206,84]。

几十年来，人们都知道任何函数都可以安全地计算，即使是在存在恶意对手的情况下[208]。虽然存在通用解决方案，但它们的性能特征往往使它们不适用于实际情况。因此，研究的一个显著趋势是为应用设计定制协议，如线性与逻辑回归[359,194,351]和神经网络训练与推理[351,10,48]。这些工作通常是在跨竖井设置中，或者计算被委托给一组互不协作的计算服务器的变体中。将这些协议移植到跨设备设置并不简单，因为它们需要大量的通信。

***同态加密*** 同态加密(HE)方案允许对密文直接进行某些数学运算，而无需事先解密。同态加密可以是启用MPC的一个强大工具，它允许参与者对值计算函数，同时隐藏值。

存在着不同的HE，从一般的完全同态加密(FHE)[197]到更有效的级别变体[87,182,88,129]，其中有几种实现[233,409,364,415,1]。同样具有实际意义的还有所谓的部分同态格式，例如ElGamal和Paillier，它们允许同态的加法或乘法。附加HE作为MPC协议中跨筒仓设置的组成部分[359,224]。[404]综述了一些同态加密软件库，并简要说明了选择库时应考虑的标准/特征。

当考虑在FL设置中使用HE时，立即出现的问题是谁持有该方案的秘钥。虽然每个客户端加密他们的数据并将其发送到服务器上进行同构计算的想法很吸引人，但是服务器不应该能够解密单个客户端贡献的数据。解决这个问题的一种简单方法是依赖一个持有秘密密钥并解密计算结果的非合谋外部方。然而，大多数HE方案需要经常更新密钥(例如，由于易受选择的密文攻击[117])。此外，可信任的非合谋方的可用性在FL方法中不是标准的。

另一种解决此问题的方法是依赖于分布式(或阈值)加密方案，其中秘钥分布在各方之间。Reyzin等人[392]和Roth等人[398]提出了在跨设备设置中计算总和的解决方案。他们的协议使用了加法同态格式(分别是ElGamal格式和基于格的格式的变体)。

**可信执行环境** 可信执行环境(TEE，也称为安全区域)可以提供机会将联邦学习过程的一部分移动到云中的可信环境中，可以对其代码进行测试和验证。

TEE可以提供几个关键的设施，以建立对代码单元已忠实而秘密地执行的信任[437]：

- 机密性：代码的执行状态仍然是秘密的，除非代码显式地发布消息。
- 完整性：代码的执行不会受到影响，除非代码显式地接收输入。
- 测量/认证：TEE可以向远程方证明什么代码(二进制)正在执行，以及它的起始状态，定义机密性和完整性的初始条件。

TEE已经以多种形式实例化，包括Intel的SGX支持的GPU[241,134]、Arm的TrustZone[281,22]和RISC-V上的Sanctum[135]，每一种都有不同的系统提供上述设施的能力。

目前的安全区域在内存方面受到限制，并且只提供对CPU资源的访问，也就是说，它们不允许在GPU或机器学习处理器上进行处理(Tramèr和Boneh[447]探索如何将TEEs与GPU结合起来进行机器学习推理)。此外，对于TEE(特别是那些在共享微处理器上运行的TEE)来说，完全排除所有类型的侧信道攻击是具有挑战性的[458]。

虽然安全区域为在其中运行的所有代码提供了保护，但在实践中还必须解决其他问题。例如，经常需要将在区域中运行的代码结构化为数据无关过程，这样它的运行时和内存访问模式就不会显示它所计算的数据的信息(参见示例[73])。此外，度量/认证通常只能证明特定的二进制文件正在运行;由系统架构师提供一种方法来证明该二进制文件具有所需的隐私属性，这可能要求使用来自开放源代码的可复制过程来构建该二进制文件。

如何跨安全区域、云计算资源和客户端设备划分联邦学习功能仍然是一个有待解决的问题。例如，安全区域可以执行关键功能，如安全聚合或重组，以限制服务器对原始客户端贡献的访问，同时将大多数联邦学习逻辑保持在可信计算基础之外。

**感兴趣的安全计算问题** 虽然安全多方计算和可信执行环境为分布式私有数据上的任何功能的私有计算问题提供了通用解决方案，但是当专注于特定的功能时，许多优化是可能的。下面描述的任务就是这种情况。

***安全聚合*** 安全聚合是针对n个客户机和一个服务器的功能。它允许每个客户端提交一个值(通常是FL设置中的向量或张量)，这样服务器只学习客户端值的聚合函数，通常是总和。

在单服务器设置(通过加性屏蔽[8,213,80,58,428]，通过阈值同态加密[417,218,103]，以及通过通用安全多方计算[94])和多个非合用服务器设置[78,27,130]中，有丰富的文献探讨了安全聚合。安全聚合也可以使用可信执行环境(如上所述)实现，如[308]。

*安全变换* 安全变化是n个客户端和一个服务器的功能。它允许每个客户端提交一个或多个消息，这样服务器只会从所有客户端学到无序的消息集合(多集)，仅此而已。具体来说，除了消息本身包含的信息外，服务器无法将任何消息链接到其发送方。安全变换可以看作是安全聚合的一个实例，其中的值是多集单例，聚合操作是多集和，尽管通常情况下，在安全变换和安全聚合的典型操作机制中，非常不同的实现提供了最佳性能。

安全变换已经在安全多方计算的背景下被研究[107,288]，通常在混合网络的标题下。它们也在可信计算环境中进行了研究[73]。混合网络以Tor网络的形式得到了大规模部署[157]。

*私有信息检索* 私有信息检索(PIR)是一个客户端和一个服务器的功能。它使客户机能够从服务器托管的数据库中下载条目，这样服务器就不会获得客户机请求的条目的任何信息。

PIR的MPC方法分为两大类：计算PIR (computing PIR，简称cPIR)，其中一方可以执行协议的整个服务器端[286];信息理论PIR (information theory PIR，简称itPIR)，其中多个非协作方需要执行协议的服务器端[121]。

PIR适用性的主要障碍如下：PIR具有较高的计算成本[423]，而非协合方设置在工业场景中难以令人信服地实现。关于PIR的最新研究结果表明，通过使用基于格的密码系统，可以显著降低计算成本[12,363,13,23,198]。计算成本可以用来换取更多的通信;为了更好地理解cPIR提供的通信和计算权衡，我们建议读者参考Ali等人的[16]。此外，它还演示了如何利用用户可用的端信息(例如通过客户端本地状态)在单个服务器上构建高效的通信PIR。Patel等人[372]提出并实现了一个实际的混合(计算和信息理论)PIR方案，该方案在假设客户端状态的单个服务器上。Corrigan-Gibbs和Kogan[131]通过离线/在线模型提出了具有次线性在线时间的PIR的理论结构，在脱机阶段，客户端从服务器获取信息，这些信息独立于将来要执行的查询。

进一步的工作探索了PIR和秘密共享之间的连接[479]，最近的连接是toPIRon编码数据[159]和通信效率PIR[72]。一个变体PIR，称为PIR-with- default，允许客户端检索一个默认值，如果查询的索引不在数据库中，并且可以输出可作为任何MPC协议的输入的附加秘密共享项[297]。PIR也在ON-OFF隐私中进行了研究，客户端被允许关闭其隐私保护以换取更好的效用或性能[355,494]。

### 4.2.2 保护隐私披露

最先进的量化和限制个人信息披露的模型是差分隐私(DP)[167,164,165]，其目的是在发布的模型中引入一定程度的不确定性，足以掩盖任何个人用户的贡献。差分隐私通过隐私损失参数(ε， δ)进行量化，其中较小的(ε， δ)对应着增加的隐私。更正式地说，如果对所有$S\subseteq Range(A)$以及所有邻近数据集D和$D^{'}$，则随机算法A是(ε， δ)-差分私有的：
$$

P\(A\(D\)\epsilon S\) \le e^{\varepsilon}P\(A\(D^{'}\)\epsilon S\)+\delta

$$ 如果$D^{'}$可以通过添加或减去单个客户端\(用户\)的所有记录从D中获得，则FL、D和$D^{'}$的上下文对应的是相邻的分散数据集\[338\]。这种差异隐私的概念称为用户级差异隐私。它比通常使用的邻接概念\(D和$D^{'}$只相差一个记录\[165\]\)更强，因为一般来说，一个用户可能会向数据集贡献许多记录\(例如训练示例\)。

在过去的十年中，已经开发了一套广泛的技术用于不同的私有数据分析，特别是在集中式设置的假设下，即在应用必要的干扰以实现隐私之前，原始数据由可信方收集。在联邦学习中，编排服务器通常充当DP机制的可信实施者，确保只将私有输出发布给模型工程师或分析人员。

然而，如果可能的话，我们通常希望减少对可信方的需求。近年来，人们考虑了几种减少对数据管理员信任需求的方法。

**本地差分隐私** 通过让每个客户机在与服务器共享数据之前对其数据应用差分隐私转换，可以在不需要信任集中式服务器的情况下实现差异隐私。也就是说，我们将公式\(3\)应用于处理单个用户的本地数据集D的机制A，并对任何可能的其他本地数据集D'保持保证。该模型被称为本地差分隐私\(LDP\)\[475,266\]。LDP已经被有效地部署在谷歌、苹果和微软的大用户群中收集流行物品的统计数据\[177,154,155\]。它还被Snap\[378\]用于垃圾分类器训练的联邦设置中。这些LDP部署都涉及到大量的客户端和报告，在Snap的情况下甚至多达10亿个，这与集中式的DP实例化形成了鲜明的对比，而DP可以从更小的数据集提供更高的实用性。不幸的是，正如我们将在第4.4.2节中讨论的那样，在维护实用性的同时实现LDP是困难的\[266,455\]。因此，需要一个在纯粹的中心DP和纯粹的局部DP之间插入的差分隐私模型。这可以通过分布式差异隐私或混合模型来实现，如下所述。

**分布式差分隐私** 为了恢复中央DP的一些效用，而不必依赖于可信任的中央服务器，可以使用分布式差分隐私模型\[166,417,73,120\]。在此模型下，客户端首先计算并编码最小\(特定于应用程序\)的报告，然后将编码后的报告发送到一个安全的计算函数，其输出可用于中央服务器，这样做的目的是，当服务器能够检查它时，该输出已经满足了不同的隐私要求。编码是为了帮助维护客户端的隐私，例如可以包括LDP。安全计算功能可以有多种表现形式。它可以是MPC协议，在TEE上完成的标准计算，或者甚至是两者的组合。每种选择都带有不同的假设和威胁模型。

值得注意的是，分布式差分隐私和本地差分隐私从几个方面产生了不同的保证：虽然分布式DP框架可以为与LDP相同级别的差异隐私产生更准确的统计数据，但它依赖于不同的方法，通常会做出更强的假设，比如对MPC协议的访问。下面，我们概述了依赖于安全聚合和安全变换的两种分布式差分隐私的可能方法。我们强调有许多其他方法可以使用，例如\[400\]一种基于在安全信道中交换相关高斯噪声的方法。

_通过安全聚合的分布式DP_ 在FL中实现分布式DP的一个很有前途的工具是安全聚合，上面在第4.2.1节中讨论过。安全聚合可以确保中心服务器获得聚合结果，同时保证各个设备和参与者的中间参数不会泄露给中心服务器。为了进一步确保聚合的结果不会向服务器透露额外的信息，我们可以使用本地差分隐私\(例如使用中等的ε级别\)。例如，每个设备可以在安全聚合之前扰动自己的模型参数，以实现本地差分隐私。通过正确设计噪声，我们可以确保聚合结果中的噪声与可信服务器\(例如具有低ε /高隐私级别\)集中添加的噪声相匹配\[8,385,205,417,213\]。

_基于安全变换的分布式DP_ 另一种分布式差异隐私模型是变换模型，它由最近引入的Encode-Shuffle-Analyze \(ESA\)框架启动[73](如图3所示)。在该框架的最简单版本中，每个客户端在其数据上运行一个LDP协议\(例如，有一个适中的ε水平\)，并将其输出提供给一个安全的变换器。变换器随机排列报表，并将经过变换器的报表集合\(没有任何标识信息\)发送给服务器进行最终分析。直观地说，这个安全计算函数的插入使得服务器很难了解参与者的任何信息，并支持差分隐私分析\(例如低ε /高隐私级别\)。在更通用的多消息变换框架中，每个用户可以向变换器发送多个消息。变换器可以作为一个独立于服务器的可信实体直接实现，也可以通过上面讨论的更复杂的密码原语实现。

Bittau等人\[73\]提出了Prochlo系统作为实施ESA框架的一种方式。该系统对隐私采取了一种整体的方法，考虑了安全计算方面\(使用TEE解决\)、隐私披露方面\(通过不同的隐私解决\)和可验证方面\(使用安全区域认证功能缓解\)。

更普遍的是，差分隐私的变换模型可以使用更广泛的本地随机化器，甚至可以自适应地选择这些本地随机化器\[178\]。这可以在依赖于较弱的信任假设的情况下，使用比本地模型中可能出现的更小的错误来启用差分私有协议比中央模型，例如，\[120,178,45,201,204,200,202,203,110\]。

**混合微分隐私** 另一种有前途的方法是混合差异隐私\[40\]，它根据用户的信任模型偏好\(例如对管理员的信任或缺乏信任\)对用户进行划分，从而组合多个信任模型。在混合模型之前，有两种自然选择。第一种是使用信任度最低的模型，它通常提供最低的实用程序，并保守地在整个用户群中统一应用它。第二种方法是使用最可信的模型，它通常提供最高的实用性，但只适用于最可信的用户。通过允许多个模型共存，与纯粹的本地或中央DP机制相比，混合模型机制可以从给定的用户基础上获得更多的效用。例如，\[40\]描述了一个系统，其中大多数用户在本地隐私模型中贡献他们的数据，而一小部分用户选择在中央DP模型中贡献他们的数据。在某些情况下，这种机制的性能优于适用于所有用户的保守的本地DP机制，也优于仅适用于一小部分选择加入用户的中央DP机制。\[57\]最近的工作进一步证明，多个信任模型的组合可以成为设计和实现差异隐私的一个有前途的工具包的一部分。这种结构可以直接应用于联邦学习环境中;然而，结合信任模型或计算模型的一般概念也可能激发类似的但新的联邦学习方法。

#### 4.2.3 可验证性

与上述隐私技术正交的一个重要概念是可验证性。一般来说，可验证计算将使一方能够向另一方证明，它已经忠实地对其数据执行了期望的行为，而不会损害数据的潜在机密性。可验证计算的概念最早可以追溯到Babai等人的\[42\]，并在文献中对各种术语进行了研究：检查计算\[42\]，认证计算\[343\]，委托计算\[210\]，以及可验证计算\[195\]。

在FL中，可验证性可用于两个目的。首先，它将使服务器能够向客户端证明它忠实地执行了预期的行为\(例如，聚合输入，变换输入消息，或为不同的隐私添加噪声\)。第二，它将使客户端能够向服务器证明他们的输入和行为遵循协议规范\(例如，输入属于某个特定的范围，或数据是正确生成的密文\)。

多种技术可以用于提供验证：零知识证明\(ZKP\)、可信执行环境\(TEE\)或远程认证。在这些ZKP中，基于数学硬度提供正式的加密安全保证，而其他ZKP则依赖于对可信硬件安全性的假设。

零知识证明\(ZKP\) 零知识\(ZK\)证明是一种加密原语，它使一方\(称为证明者\)能够向另一方\(称为验证者\)证明声明，该证明依赖于证明者\(称为证人\)已知的秘密信息，而不会向验证者泄露这些秘密。零知识的概念是由Goldwasser等人在1980年代后期提出的\[209\]。为解决私有数据的可验证性问题提供了一种解决方案。尽管有大量的ZK建设工作,第一个工作带来ZKPs和可核查的计算一般功能的实用性是Parno等人\[369\]介绍了第一次优化建设和实现简洁ZK。现在，ZKP协议可以实现数百字节的证明大小和毫秒级的验证，而不管被证明语句的大小。

ZKP有三个显著的特性：完整性\(如果该声明是真实的验证和验证人遵守协议,验证人将接受证明\)、稳定性\(如果声明是错误的和验证人遵循协议,验证人会拒绝证明\),和0知识\(如果该声明是真的和验证方的协议,验证者只会知道声明是真实的，不会从交互中了解到任何机密信息\)。

除了这些公共属性之外，还存在不同类型的零知识结构，根据支持的语言来证明、设置要求、证明者和验证者计算效率、交互性、简洁性和潜在的硬度假设。有许多ZK结构支持特定的语句类，Schnorr证明\[408\]和Sigma协议\[147\]就是这样广泛使用的协议的例子。虽然这样的协议在特定的设置中有许多用途，但是可以支持任何功能的通用ZK系统提供了一个适用范围更广的工具\(包括在FL上下文中\)，因此我们在接下来的讨论中将重点关注这样的结构。

不同结构之间的一个主要区别特征是需要可信的设置。一些ZKP依赖于公共引用字符串\(CRS\)，它使用应该保持隐藏的秘密进行计算，以保证证明的可靠性。这种CRS的计算称为可信设置。虽然这一要求对于这样的系统来说是一个缺点，但是实现最简洁的证明和验证器效率的现有ZKP结构需要可信的设置。

影响适用性的另一个重要属性在不同的场景中生成证明是否需要验证和验证者之间的相互作用,这里我们区分非交互式零知识证明\(NIZKs\),使验证方发送一个消息到校验和不需要进一步沟通。通常，我们可以通过对哈希函数的理想功能做出更强的假设\(例如，哈希函数的行为就像随机的oracle\)来将交互式证明转换为非交互式证明。

此外，对于ZKP系统的效率有不同的测量方法，例如证明的长度和验证者和验证者的计算复杂度。对于被评估的功能，理想的验证程序的复杂性在执行时间上应该是线性的，但是许多现有的ZKP为验证程序引入了额外的\(有时是显著的\)开销。最有效的验证复杂性要求计算的输入大小至少是线性的，并且在FL服务器工作的证明设置中，这个输入大小将是显著的。

简明非交互零知识证明\(SNARKs\)\[71\]是一种ZKP，它提供不变的证明大小和验证，仅依赖于输入大小，线性。这些具有吸引力的效率特性是以更强的假设为代价的，这在大多数情况下是固有的，在所有现有方案中都是可信的。大多数现有的SNARK结构利用了二次算术程序\[196,369,136\]，现在可以在开源库中获得，如libsnark\[307\]，并部署在加密货币中，如Zcash\[62\]。注意，SNARK系统通常需要验证方的开销;特别地，证明程序的计算需要在被证明的语句的电路大小上是超线性的。最近，Xie等人\[489\]提出了一种ZKP系统Libra，该系统实现了线性证明复杂性，但增加了证明规模和验证时间。

如果我们放宽对结构的简便性或非交互性的要求，就会有大量的结构实现广泛的效率权衡，避免可信设置要求，并使用更标准的加密假设\[92,464,20,63\]。

近年来，越来越多的实际应用开始使用非交互式零知识证明，这主要是由区块链驱动的。在FL环境中有效地使用交互式ZKP系统和NIZKs仍然是一个具有挑战性的开放性问题。在这样的情况下，NIZKs可以向服务器属性证明客户端的输入。在验证者是客户端的情况下，创建一个值得信任的语句来验证将是一个挑战，因为它涉及到来自其他客户端的输入。有趣的是，最近的工作能够处理多个验证者共享语句的情况\[83\]。

**可信的执行环境和远程认证** 我们在第4.2.1节中讨论了TEE，但这里的重点是TEE可以提供提供可验证计算的机会。实际上，TEE允许验证在其环境中运行的代码\(二进制\)。特别是,当验证人知道\(或复制\),二进制应该运行在安全区域,t恤将能够提供一个完整的概念\(不能影响代码的执行,除了输入\),和一个认证\(TEE可以证明特定的二进制文件正在执行，以及启动状态是什么\)\(437、451\)。更一般地说，远程认证允许验证者安全地测量远程硬件平台的内部状态，并可用于建立静态或动态信任根。虽然TEE使基于硬件的远程认证成为可能，但文献中提出了基于软件的远程认证\[411\]和混合远程认证设计\[172,274\]，并使硬件需求能够被验证。

在联邦学习方法中，TEE和远程认证对于客户机能够有效地验证服务器上运行的关键功能可能特别有帮助。例如，安全聚合或洗码可以在TEE中运行，并对其输出提供不同的隐私保证。因此，随后由服务器对差分私有数据应用的后处理逻辑可以在服务器上运行，并且对客户机保持无关。请注意，这样的系统设计要求客户端知道并信任要在飞地中应用的关键函数的确切代码\(二进制\)。此外，远程认证可能使服务器能够验证来自FL计算中涉及的客户端的特定需求，例如无泄漏、不变性和不可中断性\(对于远程认证的最低要求，我们遵循\[188\]的详尽列表\)。

### 4.3 保护不受外部恶意攻击者侵犯

在本节中，我们假设存在一个可信的服务器，并讨论实现针对外部恶意行为者\(例如，敌对客户端、敌对分析师、使用学习模型的敌对设备或它们的任何组合\)的严格隐私保证的各种挑战和开放问题。

在表7中讨论的，恶意客户端可以检查从服务器接收到的所有消息\(包括模型迭代\)，在他们参与的轮中，恶意分析者可以检查使用不同超参数的多个训练运行的模型迭代序列，而在跨设备FL中，恶意设备可以使用白盒或黑盒访问最终模型。因此，为了对外部对手提供严格的保护，首先考虑可以从中间迭代和最终模型中学到什么是很重要的。

#### 4.3.1 审核迭代和最终模型

为了更好地理解可以从中间迭代或最终模型中学到什么，我们建议量化联邦学习模型对特定攻击的易感性。这在联邦学习环境中是一个特别有趣的问题。一方面，对手从服务器直接访问模型，这扩大了攻击范围。另一方面，服务器确定对手将在训练过程的哪个特定阶段获得对模型的访问权，并在每个阶段控制对手对模型的影响。

对于经典的\(非联邦\)计算模型，了解模型对攻击的易感性是一个活跃且具有挑战性的研究领域\[189,418,99,341,100\]。量化模型易受攻击程度的最常用方法是使用与实际中预期的数据集类似的代理\(审计\)数据集来模拟对模型的攻击。如果代理数据集确实与最终的用户数据相似，这就给出了模型预期易受攻击的程度的概念。一个更安全的方法是确定模型的攻击易感性的最坏情况上界。这可以像在\[496\]中那样从理论上进行处理，尽管这通常会为现实模型产生松散、空洞的边界。经验方法可能提供更严格的界限，但对于许多类型的攻击和模型，这一努力可能是棘手的。这一领域中一个有趣的新兴研究领域考察了\(关于审计模型和攻击\)的理论条件，在这种条件下，通过模拟攻击不成功地识别隐私侵犯意味着没有更强的攻击能够成功完成这一任务\[153\]。然而，这一领域仍处于起步阶段，需要做更多的工作来更好地理解基本需求，在这种需求下审计\(通过模拟攻击\)就足够了。

联邦学习框架不仅为攻击提供了独特的方法，而且为攻击的量化和防御提供了独特的方法。具体来说，由于服务器可以控制每个用户在训练过程中何时可以访问和影响模型，因此可以设计新的易于处理的方法来量化模型的平均情况或最坏情况的攻击易感性。这样的方法将使新的自适应防御的发展成为可能，它可以在最大化效用的同时实时应用于先发制人的重大对抗影响。

### 4.3.2 中央差分隐私培训

为了限制或消除可以从迭代\(和/或最终模型\)中了解到的关于个体的信息，可以在FL的迭代训练过程中使用用户级差异隐私\[3,338,336,68\]。使用这种技术，服务器剪辑个人更新的$l\_2$范数，聚合剪辑后的更新，然后添加高斯噪声到聚合。这确保迭代不会过度适合任何单个用户的更新。为了跟踪各个回合的总体隐私预算，可以使用高级合成定理\[168,254\]或\[3,346,348,474\]中发展的分析矩会计方法。矩量计算方法特别适用于均匀下采样高斯机制。对于适度的隐私预算和缺乏足够大的数据集\[384\]，该过程引入的噪声会导致模型精度的大幅下降。之前的工作探索了许多途径来减轻隐私性和准确性之间的权衡，包括收集更多的隐私数据\[338\]，设计隐私友好的模型架构\[367\]，或利用隐私数据领域的优先性\[449\]。

在跨设备FL中，训练示例的数量在不同设备之间可能有很大差异。因此，与最近在中心模型\[21\]中关于用户级DP的研究类似，如何自适应地绑定用户贡献并剪切模型参数仍然是一个有趣的研究方向\[446,377\]。更广泛地说，与记录级DP不同的是，在各种规范学习和估计任务中，人们很好地理解了准确性和隐私之间的基本权衡，用户级DP从根本上就不那么容易理解\(特别是当贡献的数量在不同用户之间变化很大，并且没有预先严格限制时\)。因此，需要做更多的工作来更好地理解在这个新兴的DP环境中基本的权衡。最近，\[320\]通过描述在用户级DP下学习离散分布的准确性和隐私之间的权衡，在这方面取得了进展。

除了上面提到的，重要的是区分可以在培训期间看到\(部分\)中间迭代的恶意客户端和只能看到最终模型的恶意分析师\(或部署\)。尽管中央DP提供了针对这两种威胁模型的保护，但仔细的理论分析可以揭示，对于上述高斯机制\(或任何其他差分隐私机制\)的具体实现，我们可能会得到这两种威胁模型的不同隐私参数。当然，我们应该对恶意分析师提供比恶意客户端更强的差分隐私保证\(因为恶意客户端可能比恶意分析师获得更多的信息\)。Feldman等人\[185\]最近针对凸优化问题研究了这种“通过迭代实现隐私放大”的设置。然而，目前尚不清楚\[185\]的结果是否可以应用于非凸的情况。

**非均匀设备采样程序的隐私放大** 在跨设备FL系统环境中提供正式的\(ε， δ\)保证是特别具有挑战性的，因为：\(a\)所有符合条件的用户\(即底层数据库\)的集合是动态的，并且事先不知道;\(b\)参与联邦计算的用户可能在协议中的任何一点退出。因此，研究和设计以下协议是很重要的:\(1\)强大的自然选择\(用户可用性和辍学\),\(2\)self-accounting,服务器可以计算一个紧\(δε\)保证只使用可用的信息通过协议,\(3\)依靠本地参与决定\(即不假定服务器知道哪些用户在线和有能力样本\),\(4\)实现良好的隐私效用权衡。尽管最近的研究\[47,257\]表明这些约束可以同时实现，但构建一个在生产FL系统中工作的端到端协议仍然是一个重要的开放性问题。

**随机性的来源\(来自\[336\]\)** 大多数计算设备只能访问很少的熵源，而且它们的速率往往非常低\(硬件中断、板上传感器\)。使用熵来生成一个加密安全的伪随机数生成器\(PRNG\)，并在需要时使用PRNG的输出，这是标准的——理论上也是合理的。基于标准密码原语的健壮高效的PRNG存在于现代cpu上，它的输出速率为gb / s，需要的种子长度为128位\[401\]。

只要区分器是有计算界的，那么访问PRNG的随机算法A的输出分布与访问真实熵源的随机算法A的输出分布是无法区分的。与它相比，无论对手多么强大，它都能保证不同的隐私。因此，几乎所有差分隐私的实现都只满足\[347\]引入的计算差异隐私的\(变种\)。从积极的方面来看，一个有计算边界的对手并不能区分这两者，这让我们可以避免在这一点上过于学究。

一个训练过程可能有多个非确定性来源\(例如，辍学层或生产模型的输入\)，但只有反映在隐私账本中的那些必须来自加密安全的PRNG。特别是，设备采样程序和加性高斯噪声必须从一个加密安全的PRNG中提取，以便训练模型满足计算差分隐私。

**审计差分隐私实现** 隐私和安全协议的正确实现是出了名的困难\(例如，对于差异隐私，\[345,217\]\)。可以使用什么技术来测试fl实现的正确性？由于这些技术经常被选择不使用开源代码的组织所使用，黑盒测试的可能性有多大？一些著作\[156,315,247\]开始在差异隐私的背景下探索这一领域，但仍存在许多悬而未决的问题。

#### 4.3.3 隐藏迭代

在典型的联邦学习系统中，假设模型迭代\(即每轮训练后模型的最新更新版本\)对系统中的多个参与者是可见的，包括被选中参与每轮训练的服务器和客户机。但是，可以使用第4.2节中的工具来对这些参与者隐藏迭代。

为了对客户机隐藏迭代，每个客户机可以在提供机密特性的TEE中运行联邦学习的本地部分\(见第4.2.1节\)。服务器将验证预期的联邦学习代码在TEE中运行\(依赖于TEE的认证和完整性特性\)，然后向设备发送一个加密的模型迭代，这样它只能在TEE中解密。最后，在将模型更新返回到服务器之前，将在TEE中对其进行加密，使用仅在enclave中和服务器上已知的密钥。不幸的是，TEE通常不能跨客户端使用，特别是当这些客户端是终端用户设备\(如智能手机\)时。此外,即使存在TEE,他们可能没有足够强大的支持训练计算,这将发生在TEE为了保护模型进行迭代,并可计算昂贵和/或需要大量的RAM——尽管TEE功能可能会改善随着时间的推移,而诸如\[447\]中提出的技术可以通过在TEE之外导出部分计算，同时保持整个计算的认证、完整性和机密性需求，从而减少对TEE的要求。

MPC模型也可以实现类似的保护\[351,10\]。例如，服务器可以在将迭代器的模型参数发送给客户端之前，使用只有服务器知道的密钥，在同态加密方案下对其进行加密。然后，客户机可以使用密码系统的同态属性计算加密的模型更新，而不需要解密模型参数。然后可以将加密的模型更新返回给服务器进行聚合。这里的一个关键挑战是在解密之前强制在服务器上进行聚合，否则服务器可能会了解客户机的模型更新。这里的另一个具有挑战性的开放式问题是性能的提高，因为即使是最先进的系统也可能需要相当多的计算资源来完成深度神经网络的单轮训练。这方面的进展可以通过算法的改进和MPC更高效硬件加速器的开发来实现\[393\]。

如果还应该对服务器隐藏模型迭代，则会出现其他挑战。在TEE模型下，联邦学习的服务器部分可以在TEE中运行，所有各方\(即客户端和分析师\)验证服务器TEE只会在满足适当的培训标准后发布最终模型。在MPC模型下，一个加密密钥可以保护模型迭代，密钥由分析师持有，在客户端中分配，或由可信的第三方持有;在这种设置中，需要密钥持有人参与模型参数的解密，从而可以确保这个过程只发生一次。

#### 4.3.4 对变化数据的重复分析

对于联邦学习的许多应用程序，分析师希望分析以流方式到达的数据，并且必须提供动态更新的已学习模型，使其：\(1\)迄今为止看到的数据正确，\(2\)准确预测未来到达的数据。在没有隐私问题的情况下，一旦新数据到达，分析师可以简单地重新训练学习过的模型，以确保在任何时候都达到最大的准确性。然而，由于隐私保证随着关于相同数据的附加信息的发布而降低\[167,168\]，这些更新必须降低频率，以仍然保持整体分析的隐私性和准确性。

动态数据库和时间序列数据的差分隐私的最新进展\[143,142,97\]都假设存在一个可信任的管理员，他可以在原始数据在线时查看它们，并发布动态更新的统计数据。一个悬而未决的问题是，如何将这些算法技术扩展到联邦设置，以支持对时间序列数据或其他动态发展的数据库的私有联邦学习。

具体的开放式问题包括：

* 在有新数据的情况下，分析师应如何私下更新FL模型？或者，通过FL在数据集D上私下学习的模型如何扩展到在给定的紧密度度量中保证与D相似的数据集D'？由于FL已经发生在在线到达的样本上，并且没有过拟合它所看到的数据，因此这种模型很可能在新的数据库D'上仍然表现良好。这也与第5节中讨论的健壮性问题有关。
* 解决隐私构成问题的一种方法是生成合成数据\[165,5\]，这样就可以无限期地使用，而不会造成额外的隐私损失。这源于差分隐私的后处理保证\[167\]。Augenstein等人探索了以联邦方式生成合成数据的方法。在动态数据设置中，合成数据可以重复使用，直到它对于新数据已经“过时”，并且必须更新。即使在以联邦方式生成数据之后，也必须以私有的方式对其进行联邦更新。
* 以前研究动态数据库差异隐私的具体方法\[142\]或秘密检测时间序列数据的变化\[143,97\]是否可以扩展到联邦设置？
* 首先，如何在联邦模型中查询时间序列数据？按照设计，相同的用户不会定期多次查询更新的数据点，因此很难收集对个人数据随时间变化的真实的主体内估计。时间序列数据的统计抽样的常用工具可以在这里使用，但必须与隐私工具和联邦工具一起使用。其他方法包括重新定义查询，这样每个主题内的子查询都可以完全在设备上回答。

#### 4.3.5 防止模型被盗和误用

在某些情况下，开发ML模型的参与者或组织可能会受到动机的驱使，限制检查、滥用或窃取模型的能力。例如，限制对模型参数的访问可能会使对手更难搜索漏洞，比如产生意外的模型输出的输入。

在推理期间保护已部署的模型与在培训期间向客户隐藏模型迭代的挑战密切相关，如第4.3.3节所述。同样，TEE和MPC都可以使用。在TEE模型下，模型参数只能被设备上的TEE访问，如第4.3.3节所示;主要的区别在于，所需的计算现在是推理而不是训练。

如果不放弃设备上推理提供的优势，将MPC策略应用于此用例将更加困难：如果用户数据、模型参数和推理结果都是设备上的，那么就不清楚有哪一方参与了多方计算。例如，试图使用同态加密将要求解密密钥在将要使用推断的设备上，因此首先就破坏了加密的价值。需要分析师参与的解决方案\(例如，持有加密密钥或模型参数本身\)意味着对最终用户的额外推理延迟、带宽成本和连接需求\(例如，推理将不再适用于飞机模式的设备\)。

需要注意的是，即使模型参数本身被成功隐藏，研究表明，在许多情况下，只能访问基于这些参数的推理/预测API的对手可以重构它们\[450\]。对于驻留在数百万或数十亿终端用户设备上的模型，需要采取哪些附加保护措施来防止此类问题的发生，这是一个有待解决的问题。

表8：各种技术及其特性。

图3：编码-变换-分析\(ESA\)框架，这里展示了4个客户端。

