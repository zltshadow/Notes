# index

从分散数据中高效学习深度网络

## 摘要

现代移动设备拥有丰富的适合学习模型的数据，可以极大地改善用户在设备上的体验。例如，语言模型可以改进语音识别和文本输入，图像模型可以自动选择好的照片。然而，这些丰富的数据通常是隐私敏感的，数量很大，或者两者都有，这可能会阻止登录到数据中心并使用传统方法在那里进行培训。我们提倡一种替代方法，将训练数据分散在移动设备上，通过聚合本地计算的更新来学习共享模型。我们将这种分散的方法称为联邦学习。

我们提出了一种基于迭代模型平均的深度网络联邦学习方法，并对5种不同的模型架构和4个数据集进行了广泛的经验评估。这些实验表明，该方法对于不平衡和非iid数据分布\(这是该设置的一个定义特征\)是健壮的。通信成本是主要的约束条件，与同步随机梯度下降相比，我们显示所需的通信轮数减少了10 - 100倍。

### 1 介绍

手机和平板电脑逐渐成为许多人的主要计算设备\[30.2\]。这些设备上强大的传感器\(包括摄像头、麦克风和全球定位系统\)，加上它们经常被携带，意味着它们可以访问前所未有的大量数据，其中大部分是私人的。从这些数据中学习到的模型具有通过支持更智能的应用程序大大提高可用性的前景，但数据的敏感性意味着将其存储在集中位置存在风险和责任。

我们研究了一种学习技术，该技术允许用户从这些丰富的数据中训练出来的共享模型中获得好处，而不需要集中存储这些数据。我们将我们的方法称为联邦学习，因为学习任务是通过参与设备\(我们称为客户机\)的松散联邦来解决的，这些设备由中央服务器协调。每个客户端都有一个从未上传到服务器的本地训练数据集。相反，每个客户机计算由服务器维护的当前全局模型的更新，并且只有这个更新被通信。这是对2012年白宫消费者数据隐私报告\[39\]提出的集中收集或数据最小化原则的直接应用。由于这些更新是专门用于改进当前模型的，因此一旦应用了它们，就没有理由存储它们。

这种方法的一个主要优点是将模型训练与直接访问原始训练数据的需要分离开来。显然，仍然需要对协调培训的服务器有一定的信任。然而，对于可以根据每个客户机上的可用数据来指定训练目标的应用程序，联邦学习可以通过将攻击面限制在设备而不是设备和云上，从而显著降低隐私和安全风险。

我们的主要贡献是：1\)将基于移动设备分散数据的训练问题确定为一个重要的研究方向；2\)选择一个可以应用于此设置的简单而实用的算法；3\)对提出的方法进行广泛的实证评估。更具体地说，我们引入了FederatedAveraging算法，它将每个客户端上的局部随机梯度下降\(SGD\)与一个执行模型平均化的服务器相结合。我们对该算法进行了大量的实验，证明它对不平衡和非iid数据分布具有鲁棒性，并且可以减少在分散数据上训练深度网络所需的通信轮数。

**联邦学习** 想象中的问题有以下特性：1\)对来自移动设备的真实数据进行训练，比对数据中心中通常可用的代理数据进行训练具有明显的优势。2\)该数据属于隐私敏感数据或数据量较大\(相对于模型的大小而言\)，因此最好不要纯粹为了模型训练\(服务于集中收集原则\)而将其记录到数据中心。3\)对于监督任务，数据上的标签可以从用户交互中自然推断出来。

许多支持移动设备智能行为的模型都符合上述标准。举两个例子，我们考虑图像分类，例如预测哪些照片在未来最有可能被多次浏览或共享；还有语言模型，它可以通过改进解码、下一个单词预测，甚至预测整个回复\[10\]来改善语音识别和触摸屏键盘上的文本输入。这两项任务的潜在训练数据\(用户拍摄的所有照片以及他们在手机键盘上输入的所有内容，包括密码、url、信息等\)可能是隐私敏感的。这些例子的分布也很可能与容易获得的代理数据集有很大的不同：聊天和文本信息中语言的使用通常与标准语言语料有很大的不同，例如维基百科和其他网络文档；人们在手机上拍摄的照片可能与典型的Flickr照片非常不同。最后，这些问题的标签是直接可用的：输入的文本是自我标记的学习语言模型，照片标签可以通过用户与他们的照片应用程序的自然交互定义\(哪些照片被删除、共享或查看\)。

这两种任务都非常适合学习神经网络。对于图像分类，前馈深度网络，特别是卷积网络，众所周知能够提供最先进的结果\[26,25\]。对于语言建模任务，循环神经网络，特别是LSTMs，已经取得了最先进的成果\[20,5,22\]。

**隐私** 与数据中心的持久化数据培训相比，联邦学习具有明显的隐私优势。即使持有一个“匿名”数据集，通过与其他数据\[37\]连接，仍然会使用户的隐私面临风险。相比之下，为联邦学习传输的信息是改进特定模型所需的最小更新\(自然，隐私好处的强度取决于更新的内容\)。更新本身可以\(也应该\)是短暂的。它们永远不会包含更多的信息比原始的训练数据\(由数据处理不平等得来\)，一般会包含得少得多。此外，聚合算法不需要更新源，因此更新可以在不识别元数据的情况下通过Tor\[7\]等混合网络或通过可信的第三方进行传输。本文最后简要讨论了联合学习与安全多方计算和差分隐私相结合的可能性。

**联邦优化** 我们将联邦学习中隐含的优化问题称为联邦优化，将其与分布式优化联系起来\(并进行对比\)。联邦优化与典型的分布式优化问题有几个关键的区别：

* **非独立相似分布（independent identically distributed\)** 数据通常基于特定用户对移动设备的使用，因此任何特定用户的本地数据集都不能代表总体分布。
* **不平衡** 类似地，一些用户会比其他人更频繁地使用服务或应用程序，从而导致本地训练数据的数量发生变化。
* **大规模分布式** 我们希望参与优化的客户端数量比每个客户端的平均示例数量要多。
* **通信受限** 移动设备经常离线或链接缓慢昂贵。

在本文中，我们的重点是优化的非iid和不平衡性，以及通信约束的关键性质。部署的联邦优化系统还必须解决大量的实际问题：随着数据的添加和删除而变化的客户机数据集；以复杂的方式与本地数据分布相关的客户可用性\(例如，来自美式英语的电话可能会在不同的时间插入，而不是来自英式英语的电话\)；客户端从不响应或发送损坏的更新。

这些问题超出了目前工作的范围；相反，我们使用一个受控制的环境，它适合于实验，但仍然解决了客户机可用性和不平衡和非iid数据的关键问题。我们假设同步更新方案在通信中进行。有一个固定的K个客户端集合，每个客户端都有一个固定的本地数据集。在每一轮的开始，选择客户端的一个随机分数C，服务器将当前的全局算法状态发送给每个客户端\(例如，当前的模型参数\)。为了提高效率，我们只选择了一小部分客户，因为我们的实验表明，增加更多客户超过某个点，回报就会递减。然后，每个选定的客户机根据全局状态及其本地数据集执行本地计算，并向服务器发送更新。然后服务器将这些更新应用到其全局状态，然后重复此过程。

当我们专注于非凸神经网络目标时，我们所考虑的算法适用于任何形式的有限和目标从

$$
\mathop{min}\limits_{w\in{R^d}}f(w) \qquad f(w)\mathop=\limits^{def}\frac1n\sum_{i=1}^nf_i(w)
$$

对机器学习的问题,我们通常采取$f\_i\(w\)=l\(x\_i,y\_i,w\)$，也就是说在例子$\(x\_i,y\_i\)$的损失预测是有模型参数w造成的。我们假设有K客户/数据分区,$P\_k$组索引的数据点在客户K, $n\_k=\|P\_k\|$。因此，我们可以将目标\(1\)改写为

$$
f(w)=\sum_{k=1}^K\frac{n_k}nF_kw \qquad F_kw=\frac1{n_k}\sum_{i\in{P_k}}f_i(w)
$$

如果将训练样本均匀地随机分布在客户端上形成划分pk，那么我们将得到$E\_{P\_k}\[F\_k\(w\)\] = f\(w\)$，其中期望是分配给固定客户端k的一组样本。这是分布式优化算法通常做出的IID假设；我们将这种不成立的情况\(也就是说，$F\_k$可能是f的任意糟糕近似值\)作为非iid设置。

在数据中心优化中，通信成本相对较小，计算成本占主导地位，最近的重点是使用gpu来降低这些成本。相比之下，在联邦优化中通信成本占主导地位——我们通常会受到1 MB/s或更少上传带宽的限制。此外，客户通常只会在充电、上网和无计量wi-fi连接时自愿参与优化。此外，我们希望每个客户端每天只参与少量的更新轮。另一方面，由于任何单个设备上的数据集与总体数据集大小相比都很小，而且现代智能手机拥有相对较快的处理器\(包括gpu\)，与许多模型类型的通信成本相比，计算基本上是免费的。因此，我们的目标是使用额外的计算，以减少训练模型所需的通信轮数。有两种主要的方法可以增加计算量：1\)增加并行度，即在每一轮通信中使用更多的客户端独立工作；并且，2\)在每个客户端上增加计算量，而不是执行像梯度计算这样的简单计算，每个客户端在每一轮通信之间执行更复杂的计算。我们研究了这两种方法，但是我们实现的加速主要是由于在客户机上使用了最低级别的并行性后，在每个客户机上添加了更多的计算。

**相关工作**  McDonald等人对感知器进行了局部训练模型迭代平均的分布式训练Povey等人发表了DNNs的识别。Zhang等人研究了一种具有“软”平均的异步方法。这些工作只考虑集群/数据中心设置\(最多16个工作者，基于快速网络的时钟时间\)，而不考虑不平衡和非iid的数据集，这些属性对于联邦学习设置是至关重要的。我们将这种算法风格适应于联邦设置，并执行适当的经验评估，这提出了与数据中心设置相关的不同的问题，需要不同的方法。

基于与我们类似的动机，Neverova等人也讨论了在设备上保存敏感用户数据的好处。Shokri和Shmatikov\[35\]的工作有几个方面的关联：他们专注于训练深度网络，强调隐私的重要性，通过在每一轮通信中只共享参数的子集来解决通信成本；然而，他们也没有考虑不平衡和非iid数据，实证评价是有限的。

在凸设置中，分布式优化和估计问题受到了极大的关注\[4,15,33\]，一些算法确实专门关注通信效率\[45,34,40,27,43\]。除了假设凸性之外，现有的工作通常要求客户机的数量要比每个客户机的示例数量少得多，数据以IID的方式分布在客户机之间，并且每个节点有相同数量的数据点-在联邦优化设置中违反了所有这些假设。SGD的异步分布式形式也被应用于训练神经网络，例如，Dean等人\[12\]，但这些方法在联邦设置中需要大量的更新。分布式共识算法\(例如\[41\]\)放宽了IID假设，但仍然不适合在很多客户端上进行通信约束优化。

我们考虑的\(参数化\)算法家族的一个端点是简单的一次性平均，其中每个客户端求解的模型使其局部数据的损失最小化\(可能是正则化的\)，这些模型被平均以产生最终的全局模型。这种方法已经在IID数据的凸情况下进行了广泛的研究，众所周知，在最坏的情况下，产生的全局模型并不比在单个客户端上训练模型更好\[44,3,46\]。

